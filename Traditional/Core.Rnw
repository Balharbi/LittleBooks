<<echo=FALSE,include=FALSE>>=
opts_chunk$set( fig.path="figure/Core-fig-" ) 
set_parent('Master-Core.Rnw')
set.seed(123)
@


<<include=FALSE>>=
require(fastR)
@

\chapter{Introduction}


In this monograph, we briefly review the commands and functions needed
to analyze data from introductory and second courses in statistics.  This is intended to complement
the \emph{Start Teaching with R} and \emph{Start Modeling with R} books.

Most of our examples will use data from the HELP (Health Evaluation and Linkage to Primary 
Care) study: a randomized clinical trial of a novel 
way to link at-risk subjects with primary care.  More information on the
dataset can be found in chapter \ref{sec:help}.


Since the selection and order of topics can vary greatly from 
textbook to textbook and instructor to instructor, we have chosen to 
organize this material by the kind of data being analyzed.  This should make
it straightforward to find what you are looking for even if you present 
things in a different order.  This is also a good organizational template
to give your students to help them keep straight ``what to do when".

Some data management is needed by students (and more by instructors).  This
material is reviewed in chapter \ref{sec:manipulatingData}.

\myindex{vignettes}%
This work leverages initiatives undertaken by Project MOSAIC (\url{http://www.mosaic-web.org}), an NSF-funded effort to improve the teaching of statistics, calculus, science and computing in the undergraduate curriculum. In particular, we utilize the
\pkg{mosaic} package, which was written to simplify the use of \R for introductory statistics courses. A short summary of the \R\ commands needed to teach introductory statistics can be found in the mosaic package vignette (\url{http://cran.r-project.org/web/packages/mosaic/vignettes/mosaic-resources.pdf}).

Other related resources from Project MOSAIC may be helpful, including an annotated set of examples
from the 
sixth edition of
Moore, McCabe and Craig's \emph{Introduction to the Practice of Statistics}\cite{moor:mcca:2007} (see \url{http://www.amherst.edu/~nhorton/ips6e}),
the second and third editions of the \emph{Statistical Sleuth}\cite{Sleuth2} (see \url{http://www.amherst.edu/~nhorton/sleuth}), 
and \emph{Statistics: Unlocking the Power of Data} by Lock et al (see \url{https://github.com/rpruim/Lock5withR}).

\myindex{installing packages}%
\Rindex{install.packages()}%
To use a package within R, it must be installed (one time), and loaded (each session). The 
\pkg{mosaic} package can be installed using the following command:
<<install_mosaic,eval=FALSE>>=
install.packages('mosaic')               # note the quotation marks
@
\TeachingTip{\Rstudio\ features a simplified package installation tab (on the bottom right panel).}
The {\tt \#} character is a comment in R, and all text after that on the
current line is ignored.

\myindex{loading packages}%
\Rindex{require()}%
Once the package is installed (one time only), it can be loaded by running the command:
<<load_mosaic,eval=FALSE>>=
require(mosaic)
@

\myindex{reproducible analysis}%
\myindex{markdown}%
\myindex{knitr}%
\marginnote{Using Markdown or \pkg{knitr}/\LaTeX\ requires that
the \pkg{markdown} package be installed on your system.}%
The RMarkdown system provides a simple markup language and renders the
results in PDF, Word, or HTML.  
This allows students to undertake their analyses using a workflow that
facilitates ``reproducibility'' and avoids cut and paste errors.

We typically introduce students to RMarkdown very early,
requiring students to use it for assignments and reports\cite{baum:2014}.
\TeachingTip{The \pkg{knitr}/\LaTeX\ system
allows users to combine \R\ and \LaTeX\ in the same document.  The
reward for learning this more complicated system is much finer control
over the output format.}%

Depending on the level of the course, students can use either of these for homework and projects.





\chapter{One Quantitative Variable}

\section{Numerical summaries}

\R\ includes a number of commands to numerically summarize variables.
These include the capability of calculating the mean, standard deviation,
variance, median, five number summary, interquartile range (IQR) as well as arbitrary quantiles.  We will
illustrate these using the CESD (Center for Epidemiologic Studies--Depression)
measure of depressive symptoms (which takes on values between 0 and 60, with higher
scores indicating more depressive symptoms).  

To improve the legibility of output,
we will also set the default number of digits to display to a more reasonable
level (see \function{?options} for more configuration possibilities).

\myindex{HELPrct dataset}%
\Rindex{options()}%
\Rindex{require()}%
\Rindex{mosaic package}%
<<cesd1,cache=FALSE>>=
require(mosaic)
options(digits=3)
mean(~ cesd, data=HELPrct)
@

\myindex{Start Teaching with R@\emph{Start Teaching with R}}%
\myindex{Teaching with R@\emph{Teaching with R}}%
\myindex{Start Modeling with R@\emph{Start Modeling with R}}%
\myindex{Modeling with R@\emph{Modeling with R}}%
Note that the \function{mean()} function in the \pkg{mosaic} package supports a modeling language
common to \pkg{lattice} graphics and linear models (e.g., \function{lm()}).  We will use
this commands using this modeling language throughout this document.  Those already familiar with \R\ may be surprised by the form of this command.
\DiggingDeeper{The \emph{Start Modeling with R} book will be helpful if you are unfamiliar with the 
modeling language.  The \emph{Start Teaching with R} also provides useful guidance in getting started.}

\Rindex{with()}%
\Rindex{mean()}%
The same output could be 
created using the following commands (though we will use the MOSAIC versions when available).
<<>>=
with(HELPrct, mean(cesd))
mean(HELPrct$cesd)
@
\Rindex{sd()}%
\Rindex{var()}%
Similar functionality exists for other summary statistics.
<<cesd2,cache=FALSE>>=
sd(~ cesd, data=HELPrct)
@
<<cesd2b,cache=FALSE>>=
sd(~ cesd, data=HELPrct)^2
var(~ cesd, data=HELPrct)
@

It is also straightforward to calculate quantiles of the distribution.

\myindex{quantiles}%
\Rindex{median()}%
<<cesd3>>=
median(~ cesd, data=HELPrct)
@

By default, the 
\function{quantile()} function displays the quartiles, but can be given
a vector of quantiles to display.  
\Rindex{quantile()}%
\Caution{Not all commands have been upgraded to
support the formula interface. For these functions, variables within data frames must be accessed using \function{with()} or the \$ operator.}
<<cesd4>>=
with(HELPrct, quantile(cesd))
with(HELPrct, quantile(cesd, c(.025, .975)))
@

\Rindex{favstats()}%
Finally, the \function{favstats()}
function in the \pkg{mosaic} package provides a concise summary of 
many useful statistics.
<<>>=
favstats(~ cesd, data=HELPrct)
@

\section{Graphical summaries}
The \function{histogram()} function is used to create a histogram.
Here we use the formula interface (as discussed in the \emph{Start Modeling with R} book) to
specify that we want a histogram of the CESD scores.

\Rindex{histogram()}%
\vspace{-4mm}
\begin{center}
<<cesd-hist>>=
histogram(~ cesd, data=HELPrct)
@
\end{center}


\Rindex{tally()}%
\Rindex{format option}%
In the \variable{HELPrct} dataset, approximately one quarter of the subjects are female.  
<<>>=
tally(~ sex, data=HELPrct)
tally(~ sex, format="percent", data=HELPrct)
@

It is straightforward to restrict our attention to just the female subjects.
If we are going to do many things with a subset of our data, it may be easiest
to make a new data frame containing only the cases we are interested in.
The \function{subset()} function can generate a new data frame containing
just the women or just the men  (see also section \ref{sec:subsets}).  Once this is created, the
the \function{stem()} function is used to create a stem and leaf plot.
\Caution{Note that the tests for equality use \emph{two} equal signs}
\Rindex{stem()}%
\Rindex{subset()}%
<<cesd-stem>>=
female <- subset(HELPrct, sex=='female')
male <- subset(HELPrct, sex=='male')
with(female, stem(cesd))
@

Subsets can also be generated and used ``on the fly" (this time including
an overlaid normal density):
\Rindex{fit option}%
<<women-cesd-hist,tidy=FALSE,message=FALSE>>=
histogram(~ cesd, fit="normal",
  data=subset(HELPrct, sex=='female'))  
@

Alternatively, we can make side-by-side plots to compare multiple subsets.
<<cesd-male-female>>=
histogram(~ cesd | sex, data=HELPrct)
@

The layout can be rearranged.
\Rindex{layout option}%
\begin{center}
<<cesd-dotlayout>>=
histogram(~ cesd | sex, layout=c(1, 2), data=HELPrct)   
@
\end{center}
\begin{problem}
Using the \dataframe{HELPrct} dataset, 
create side-by-side histograms of the CESD scores by substance abuse
group, just for the male subjects, with an overlaid normal density.
\end{problem}%
\begin{solution}
<<subsmale, tidy=FALSE>>=
histogram(~ cesd | substance, fit="normal", 
  data=subset(HELPrct, sex=='male'))
@
\end{solution}%
We can control the number of bins in a number of ways.  These can be specified 
as the total number.
\Rindex{nint option}%
\begin{center}
<<cesd-dot,fig.height=1.8>>=
histogram(~ cesd, nint=20, data=female)   
@
\end{center}
The width of the bins can be specified.
\Rindex{width option}%
\begin{center}
<<cesd-dotwidth,fig.height=1.8>>=
histogram(~ cesd, width=1, data=female)   
@
\end{center}
We could also have made our subset ``on the fly'', just for the purposes of graphing:
\begin{center}
<<cesd-dot2,fig.height=1.8>>=
histogram(~ cesd, data=HELPrct, subset=(sex=='female'))   
@
\end{center}

The \function{dotPlot()} function is used to create a dotplot
for a smaller subset of subjects (homeless females).  We also demonstrate
how to change the x-axis label.
\Rindex{dotPlot()}%
<<cesd-dot4,tidy=FALSE>>=
dotPlot(~ cesd, xlab="CESD score", 
  data=subset(HELPrct, (sex=="female") & (homeless=="homeless")))
@


\section{Density curves}

\FoodForThought{Density plots are also sensitive to certain choices.  If your density plot
is too jagged or too smooth, try adjusting the \option{adjust} argument (larger than 1 for
smoother plots, less than 1 for more jagged plots).}
One disadvantage of histograms is that they can be sensitive to the choice of the
number of bins.  Another display to consider is a density curve.

Here we adorn a density plot with some gratuitous additions to 
demonstrate how to build up a graphic for pedagogical purposes.
We add some text, a superimposed normal density as well as
a vertical line. A variety of line types and colors can be specified, 
as well as line widths.

\begin{center}
\Rindex{densityplot()}%
\Rindex{ladd()}%
\Rindex{panel.mathdensity()}%
\Rindex{panel.abline()}%
\Rindex{col option}%
\Rindex{grid.text()}%
\Rindex{lty option}%
\Rindex{lwd option}%
<<dens1,tidy=FALSE,fig.keep="last">>=
densityplot(~ cesd, data=female)
ladd(grid.text(x=0.2, y=0.8, 'only females'))
ladd(panel.mathdensity(args=list(mean=mean(cesd), 
  sd=sd(cesd)), col="red"), data=female)
ladd(panel.abline(v=60, lty=2, lwd=2, col="grey"))
@
\end{center}
\DiggingDeeper{The \function{plotFun()} function can also be used to annotate plots (see
section \ref{sec:plotFun}).}

\section{Frequency polygons}
\myindex{polygons}%

A third option is a frequency polygon, where  the graph is created by joining the midpoints of the top of the bars of a histogram.
\Rindex{freqpolygon()}%
\begin{center}
<<poly,tidy=FALSE,fig.keep="last">>=
freqpolygon(~ cesd, data=female)
@
\end{center}


\section{Normal distributions}

\FoodForThought{\code{x} is for eXtra.}%
The most famous density curve is a normal distribution.  The \function{xpnorm()} function
displays the probability that a random variable is less than the first argument, for a 
normal distribution with mean given by the second argument and standard deviation by the 
third.  
More information about probability distributions can 
be found in section \ref{sec:probability}.
\begin{center}
<<norm1,fig.width=5,fig.height=2.4>>=
xpnorm(1.96, mean=0, sd=1)
@
\end{center}

\section{Inference for a single sample}
\label{sec:bootstrapsing}

\Rindex{t.test()}%
\Rindex{confint()}%

We can calculate a 95\% confidence interval for the mean CESD 
score for females by using a t-test:
<<tinterval>>=
t.test(~ cesd, data=female)
confint(t.test(~ cesd, data=female))
@

\DiggingDeeper{More details and examples can be found in the 
\pkg{mosaic} package Resampling Vignette.}
\myindex{bootstrapping}%
\myindex{resampling}%
But it's also straightforward to calculate this using a bootstrap.
The statistic that we want to resample is the mean.  
<<calculuatemean>>=
mean(~ cesd, data=female)
@

One resampling trial can be carried out:
\TeachingTip{Here we sample with replacement from the original data frame,
creating a resampled data frame with the same number of rows.}
\Rindex{resample()}%
<<resamplemean>>=
mean(~ cesd, data=resample(female))
@
\TeachingTip{Even though a single trial is of little use, it's smart having
students do the calculation to show that they are (usually!) getting a different
result than without resampling.}

Another will yield different results:
<<>>=
mean(~ cesd, data=resample(female))
@

Now conduct 1000 resampling trials, saving the results in an object
called \texttt{trials}:
\Rindex{do()}%
\Rindex{qdata()}%
<<bootint>>=
trials = do(1000) * mean(~ cesd, data=resample(female))
qdata(c(.025, .975), ~ result, data=trials)
@

\chapter{One Categorical Variable}

\section{Numerical summaries}

\myindex{categorical variables}
\myindex{contingency tables}
\myindex{tables}
The \function{tally()} function can be used to calculate
counts, percentages and proportions for a categorical variable.

\Rindex{tally()}%
\Rindex{margins option}%
<<homeless-table>>=
tally(~ homeless, data=HELPrct)      
tally(~ homeless, margins=TRUE, data=HELPrct)      
tally(~ homeless, format="percent", data=HELPrct)      
tally(~ homeless, format="proportion", data=HELPrct)  
@

\section{The binomial test}

\myindex{binomial test}%
\Rindex{binom.test()}%
An exact confidence interval for a proportion (as well as a test of the null 
hypothesis that the population proportion is equal to a particular value [by default 0.5]) can be calculated
using the \function{binom.test()} function.
The standard \function{binom.test()} requires us to tabulate.
<<>>=
binom.test(209, 209 + 244)
@
The \pkg{mosaic} package provides a formula interface that avoids the need to pre-tally
the data.
<<binomtest>>=
result <- binom.test(~ (homeless=="homeless"), HELPrct)
result
@

As is generally the case with commands of this sort, 
there are a number of useful quantities available from 
the object returned by the function.  
<<>>=
names(result)
@
These can be extracted using the {\tt \$} operator or an extractor function.
For example, the user can extract the confidence interval or p-value.
\Rindex{confint()}%
\Rindex{pval()}%
\Rindex{print()}%
<<binomtest2>>=
result$statistic
confint(result)
pval(result)
@
\DiggingDeeper{Most of the objects in \R\ have a \function{print()}
method.  So when we get \code{result}, what we are seeing displayed in the console is
\code{print(result)}.  There may be a good deal of additional information
lurking inside the object itself.  To make matter even more complicated, some
objects are returned \emph{invisibly}, so nothing prints.  You can still assign
the returned object to a variable and process it later, even if nothing shows up
on the screen.  This is sometimes helpful for \pkg{lattice} graphics functions.}%


\section{The proportion test}

A similar interval and test can be calculated using \function{prop.test()}.
\Rindex{prop.test()}%
\Rindex{correct option}%
<<tidy=FALSE>>=
tally(~ homeless, data=HELPrct)
prop.test(~ (homeless=="homeless"), correct=FALSE, data=HELPrct)
@
It also accepts summarized data, the way \function{binom.test()} does.
\InstructorNote{\function{prop.test()} calculates a Chi-squared statistic.
Most introductory texts use a $z$-statistic.  They are mathematically equivalent 
in terms of inferential statements, but
you may need to address the discrepancy with your students.}%
<<>>=
prop.test(209, 209 + 244, correct=FALSE)
@

\section{Goodness of fit tests}

A variety of goodness of fit tests can be calculated against a reference 
distribution.  For the HELP data, we could test the null hypothesis that there is an equal
proportion of subjects in each substance abuse group back in the original populations.

\Caution{The \option{margins=FALSE} option is the default for the \function{tally} function.}
<<gofmore,tidy=FALSE>>=
tally(~ substance, format="percent", data=HELPrct)
observed <- tally(~ substance, data=HELPrct)
observed
@
\Rindex{chisq.test()}%
<<tidy=FALSE>>=
p <- c(1/3, 1/3, 1/3)   # equivalent to rep(1/3, 3)
chisq.test(observed, p=p)
total <- sum(observed); total
expected <- total*p; expected
@

We can also calculate the $\chi^2$ statistic manually, as a function of observed and expected values.

\TeachingTip{We don't have students do  much if any manual calculations in our courses.}%
\Rindex{sum()}%
\Rindex{pchisq()}%
<<gof2,tidy=FALSE>>=
chisq <- sum((observed - expected)^2/(expected)); chisq
1 - pchisq(chisq, df=2)
@
\TeachingTip{The \function{pchisq} function calculates the probability that a $chi^2$ random variable with \function{df} degrees is freedom is less than or equal to a given value.  Here we calculate the complement to find the area to the right of the observed Chi-square statistic.}%

Alternatively, the \pkg{mosaic} package provides a version of \function{chisq.test()} with
more verbose output.
\FoodForThought{\code{x} is for eXtra.}
<<>>=
xchisq.test(observed, p=p)
@
\TeachingTip{Objects in the workspace that are no longer needed can be removed.}
<<>>=
# clean up variables no longer needed
rm(observed, p, total, chisq)  
@


\chapter{Two Quantitative Variables}

\section{Scatterplots}
\myindex{scatterplots}%
\myindex{lowess}%
\myindex{smoothers}%
\myindex{linearity}%

We always encourage students to start any analysis by graphing their data.  
Here we augment a scatterplot
of the CESD (a measure of depressive symptoms, higher scores indicate more symptoms) and the MCS (mental component score from the SF-36, where higher scores indicate better functioning) for female subjects
with a lowess (locally weighted scatterplot smoother) line, using a circle
as the plotting character and slightly thicker line.

\InstructorNote{The lowess line can help to assess linearity of a relationship. This is added by specifying both points (using `p') and a lowess smoother.}
\Rindex{xyplot()}%
\Rindex{pch option}%
\Rindex{cex option}%
\Rindex{lwd option}%
\Rindex{type option}%
\begin{center}
<<HELPrct-xyplot,fig.height=3,tidy=FALSE>>=
females = subset(HELPrct, female==1)
xyplot(cesd ~ mcs, type=c("p","smooth"), pch=1, cex=0.6,
  lwd=3, data=females)
@
\end{center}


\section{Correlation}

Correlations can be calculated for a pair of variables, or for a matrix of variables.
\myindex{correlation}%
\Rindex{cor()}%
<<>>=
cor(cesd, mcs, data=females)
smallHELP = subset(females, select=c(cesd, mcs, pcs))
cor(smallHELP)
@
\myindex{Pearson correlation}%
\myindex{Spearman correlation}%

By default, Pearson correlations are provided. Other variants (e.g., Spearman) can be specified using the
\option{method} option.
<<>>=
cor(cesd, mcs, method="spearman", data=females)
@

\section{Pairs plots}
\myindex{pairs plot}%
\myindex{scatterplot matrix}%

A pairs plot (scatterplot matrix) can be calculated for each pair of a set of variables.
\TeachingTip{The \pkg{GGally} package has support for more elaborate pairs plots.}
\Rindex{splom()}%
<<fig.height=3>>=
splom(smallHELP)
@

\section{Simple linear regression}

\InstructorNote{We tend to introduce linear regression
early in our courses, as a purely descriptive technique.}
\myindex{linear regression}%
\myindex{regression}%

Linear regression models are described in detail in \emph{Start Modeling with R}.
These use the same formula interface introduced previously for numerical and graphical
summaries
to specify the outcome 
and predictors.  Here we consider fitting the model \model{\variable{cesd}}{\variable{mcs}}.


\Rindex{lm()}%
\Rindex{coef()}%
<<>>=
model <- lm(cesd ~ mcs, data=females)
coef(model)
@

To simplify the output, we turn off the option to display significance stars.
\myindex{significance stars}%
\Rindex{summary()}%
\Rindex{confint()}%
\Rindex{rsquared()}%
\Rindex{coef()}%
<<>>=
options(show.signif.stars=FALSE)
coef(model)
summary(model)
coef(summary(model))
confint(model)
rsquared(model)
@


\Rindex{class()}%
<<lmclass>>=
class(model)
@
The return value from \function{lm()} is a linear model object.
A number of functions can operate on these objects, as
seen previously with \function{coef()}.  
The function \function{residuals()} returns a
vector of the residuals.
\Rindex{residuals()}%
\FoodForThought{The function \function{residuals()} can be abbreviated 
\function{resid()}.  Another useful function is \function{fitted()}, which 
returns a vector of predicted values.}


\Rindex{density option}%
\begin{center}
<<lmhist>>=
histogram(~ residuals(model), density=TRUE)
@
\end{center}
\Rindex{qqmath()}%
\begin{center}
<<HELPrct-resid-qq>>=
qqmath(~ resid(model))
@
\end{center}
\Rindex{alpha option}%
\begin{center}
<<HELPrct-resid-plot>>=
xyplot(resid(model) ~ fitted(model), type=c("p", "smooth", "r"), alpha=0.5, cex=0.3, pch=20)
@
\end{center}

The \function{mplot()} function can facilitate creating a variety of useful plots, including the same residuals vs. fitted scatterplots, by specifying the \option{which=1} option.
\Rindex{mplot()}%
\Rindex{multiplot option}%
\Rindex{which option}%
<<>>=
mplot(model, which=1, multiplot=FALSE)
@

It can also generate a 
normal quantile-quantile plot (\option{which=2}),
<<>>=
mplot(model, which=2, multiplot=FALSE)
@

\myindex{scale versus location}%
scale vs.\,location,
<<>>=
mplot(model, which=3, multiplot=FALSE)
@

\myindex{Cook's distance}%
Cook's distance by observation number,
<<>>=
mplot(model, which=4, multiplot=FALSE)
@

\myindex{leverage}%
residuals vs.\,leverage, and
<<>>=
mplot(model, which=5, multiplot=FALSE)
@

Cook's distance vs. leverage.
<<>>=
mplot(model, which=6, multiplot=FALSE)
@

\myindex{prediction bands}%
\Rindex{panel.lmbands()}%
\Rindex{band.lwd option}%
Prediction bands can be added to a plot using the \function{panel.lmbands()} function.
\begin{center}
<<>>=
xyplot(cesd ~ mcs, panel=panel.lmbands, cex=0.2, band.lwd=2, data=HELPrct)
@
\end{center}

\begin{problem}
Using the \dataframe{HELPrct} dataset, fit a simple linear regression model 
predicting the number of drinks per day as a function of the mental
component score.  
This model can be specified using the formula:
\model{\variable{i1}}{\variable{mcs}}.  
Assess the distribution of the residuals for this model.
\end{problem}


\chapter{Two Categorical Variables}


\section{Cross classification tables}
\label{sec:cross}

\myindex{cross classification tables}%
\myindex{contingency tables}%
\myindex{tables}%

Cross classification (two-way or $R$ by $C$) tables can be constructed for
two (or more) categorical variables.  Here we consider the contingency table
for homeless status (homeless one or more nights in the past 6 months or housed) 
and sex.

<<homeless-sex>>=
tally(~ homeless + sex, margins=FALSE, data=HELPrct)     
@

We can also calculate column percentages:
\Rindex{tally()}%
<<homeless-sex-row,tidy=FALSE>>=
tally(~ sex | homeless, margins=TRUE, format="percent", 
  data=HELPrct)     
@

We can calculate the odds ratio directly from the table:
<<tidy=FALSE>>=
OR = (40/169)/(67/177); OR
@

The
\pkg{mosaic} package has a function which will calculate odds ratios:
\Rindex{oddsRatio()}%
<<tidy=FALSE>>=
oddsRatio(tally(~ (homeless=="housed") + sex, margins=FALSE, 
  data=HELPrct)) 
@

Graphical summaries of cross classification tables may be helpful in visualizing
associations.  Mosaic plots are one example, where the total area (all observations) is proportional to one.
\Caution{The jury is still out
regarding the utility of mosaic plots, relative to the low data to ink ratio\cite{Tufte:2001:Visual}.  But we have found them to be helpful to reinforce understanding of a two way contingency table.}%
Here we see that males tend to be over-represented
amongst the homeless subjects (as represented by the horizontal line which is higher for
the homeless rather than the housed).  
\FoodForThought{The \function{mosaic()} function 
in the \pkg{vcd} package also makes mosaic plots.}
\Rindex{mosaicplot()}%
\begin{center}
<<mosaicplot,fig.height=3.5,tidy=FALSE>>=
mytab <- tally(~ homeless + sex, margins=FALSE, 
  data=HELPrct)
mosaicplot(mytab)
@
\end{center}

\section{Chi-squared tests}

\Rindex{chisq.test()}%
<<chisq1,tidy=FALSE>>=
chisq.test(tally(~ homeless + sex, margins=FALSE,
  data=HELPrct), correct=FALSE)
@

There is a statistically significant association found: it is unlikely that we would observe
an association this strong if homeless status and sex were independent in the 
population.

When a student finds a significant association, 
it's important for them to be able to interpret this in the context of the problem. 
The \function{xchisq.test()} function provides additional details (observed, expected, contribution to statistic, and residual) to help with this process.
\FoodForThought{\code{x} is for eXtra.}

\Rindex{xchisq.test()}%
<<chisq2,tidy=FALSE>>=
xchisq.test(tally(~homeless + sex, margins=FALSE,
  data=HELPrct), correct=FALSE)
@

We observe that there are fewer homeless women, and more homeless men that would be expected.

\section{Fisher's exact test}
\myindex{Fisher's exact test}%

An exact test can also be calculated.  This is computationally straightforward for 2 by 2
tables.  Options to help constrain the size of the problem for larger tables exist
(see \verb!?fisher.test()!).

\DiggingDeeper{Note the different estimate of the odds ratio from that seen in section \ref{sec:cross}.
The \function{fisher.test()} function uses a different estimator (and different interval based 
on the profile likelihood).}
\Rindex{fisher.test()}%
<<help-fisher,tidy=FALSE>>=
fisher.test(tally(~homeless + sex, margins=FALSE,
  data=HELPrct))
@

\chapter{Quantitative Response to a Categorical Predictor}

\section{A dichotomous predictor: numerical and graphical summaries}
Here we will compare the distributions of CESD scores by sex.

The \function{mean()} function can be used to calculate the mean CESD score
separately for males and females.
<<aggregate>>=
mean(cesd ~ sex, data=HELPrct)
@

The \function{favstats()} function can provide more statistics by group.
<<summary>>=
favstats(cesd ~ sex, data=HELPrct)
@


Boxplots are a particularly helpful graphical display to compare distributions.
The \function{bwplot()} function can be used to display the boxplots for the
CESD scores separately by sex.  We see from both the numerical and graphical
summaries that women tend to have slightly higher CESD scores than men.

\FoodForThought{Although we usually put explanatory variables along the horizontal axis,
page layout sometimes makes the other orientation preferable for these plots.}
%\vspace{-8mm}
\begin{center}
<<cesd-box,fig.height=1.5>>=
bwplot(sex ~ cesd, data=HELPrct)
@
\end{center}

When sample sizes are small, there is no reason to summarize with a boxplot
since  \function{xyplot()} can handle categorical predictors.
Even with 10--20 observations in a group, a scatter plot is often quite readable.
Setting the alpha level helps detect multiple observations with the same value.
\FoodForThought{One of us once saw a biologist proudly present
side-by-side boxplots.  Thinking a major victory had been won, he naively
asked how many observations were in each group.  ``Four,'' replied the 
biologist.}
\begin{center}
<<KidsFeet-xy,fig.height=1.5>>=
xyplot(sex ~ length, KidsFeet, alpha=.6, cex=1.4) 
@
\end{center}

\section{A dichotomous predictor: two-sample t}

The Student's two sample t-test can be run without (default) or with an equal variance assumption.
<<HELPrct-nonpar,tidy=FALSE>>=
t.test(cesd ~ sex, var.equal=FALSE, data=HELPrct)
@
We see that there is a statistically significant difference between the two groups.

We can repeat using the equal variance assumption.
<<tidy=FALSE>>=
t.test(cesd ~ sex, var.equal=TRUE, data=HELPrct)
@

The groups can also be compared using the \function{lm()} function (also with an equal variance assumption).
<<>>=
summary(lm(cesd ~ sex, data=HELPrct))
@

\TeachingTip{While it requires use of the equal variance assumption, the \function{lm} function is part of a much more flexible modeling framework (while \function{t.test} is essentially a dead end).}%


\section{Non-parametric 2 group tests}

The same conclusion is reached using a non-parametric (Wilcoxon rank sum) test.

<<HELPrct-nonpar2>>=
wilcox.test(cesd ~ sex, data=HELPrct)
@


\section{Permutation test}
\myindex{resampling}%
\myindex{permutation test}%


Here we extend the methods introduced in section \ref{sec:bootstrapsing} to 
undertake a two-sided test comparing the ages at baseline by gender.  First we calculate the observed difference in means:
\Rindex{compareMean()}%
\Rindex{shuffle()}%
<<>>=
mean(age ~ sex, data=HELPrct)
test.stat <- compareMean(age ~ sex, data=HELPrct)
test.stat
@
We can calculate the same statistic after shuffling the group labels:
<<>>=
do(1) * compareMean(age ~ shuffle(sex), data=HELPrct)
do(1) * compareMean(age ~ shuffle(sex), data=HELPrct)
do(3) * compareMean(age ~ shuffle(sex), data=HELPrct)
@

\DiggingDeeper{More details and examples can be found in the
\pkg{mosaic} package Resampling Vignette.}
\Rindex{xlim option}%
\Rindex{groups option}%
<<permute-HELPrct,fig.keep='last', tidy=FALSE>>=
rtest.stats = do(500) * compareMean(age ~ shuffle(sex), 
  data=HELPrct)
favstats(~ result, data=rtest.stats)
histogram(~ result, n=40, xlim=c(-6, 6),
  groups=result >= test.stat, pch=16, cex=.8, 
  data=rtest.stats)
ladd(panel.abline(v=test.stat, lwd=3, col="red"))
@

Here we don't see much evidence to contradict the null hypothesis that men and 
women 
have the same mean age in the population.

\section{One-way ANOVA}
\myindex{One-way ANOVA}%
\myindex{analysis of variance}%

Earlier comparisons were between two groups. We can also consider testing differences between
three or more groups using one-way ANOVA.  Here we compare
CESD scores by primary substance of abuse (heroin, cocaine, or alcohol).

\Rindex{bwplot()}%
\begin{center}
<<cesd-oneway,fig.width=6,fig.height=1.9>>=
bwplot(cesd ~ substance, data=HELPrct)
@
\end{center}


<<aggregate2>>=
mean(cesd ~ substance, data=HELPrct)
@
\Rindex{aov()}%
<<help-aov>>=
mod <- aov(cesd ~ substance, data=HELPrct)
summary(mod)
@
While still high (scores of 16 or more are generally considered to be 
``severe'' symptoms), the cocaine-involved group tend to have lower 
scores than those whose primary substances are alcohol or heroin.
<<help-aovlm>>=
mod1 <- lm(cesd ~ 1, data=HELPrct)
mod2 <- lm(cesd ~ substance, data=HELPrct)
@

The \function{anova()} command can summarize models.
\Rindex{anova()}%
<<>>=
anova(mod2)
@

It can also be used to formally 
compare two (nested) models.
\myindex{model comparison}%
<<help-aov-more>>=
anova(mod1, mod2)
@


\section{Tukey's Honest Significant Differences}
\myindex{Tukey's HSD}%
\myindex{honest significant differences}%
\myindex{multiple comparisons}%

There are a variety of multiple comparison procedures that can be
used after fitting an ANOVA model.  One of these is Tukey's Honest
Significant Differences (HSD).  Other options are available within the 
\pkg{multcomp} package.

<<help-hsd>>=
favstats(cesd ~ substance, data=HELPrct)
@
\Rindex{TukeyHSD()}%
\Rindex{factor()}%
\Rindex{levels option}%
\Rindex{labels option}%
\Rindex{transform()}%
\Rindex{lm()}%
<<help-hsd2,tidy=FALSE>>=
HELPrct <- transform(HELPrct, subgrp = factor(substance, 
  levels=c("alcohol", "cocaine", "heroin"),
  labels=c("A", "C", "H")))
mod <- lm(cesd ~ subgrp, data=HELPrct)
HELPHSD <- TukeyHSD(mod, "subgrp")
HELPHSD
@
\Rindex{mplot()}%
<<help-hsd3,fig.height=3.5,fig.width=6>>=
mplot(HELPHSD)
@

Again, we see that the cocaine group has significantly lower CESD scores
than either of the other two groups.

\chapter{Categorical Response to a Quantitative Predictor}

\section{Logistic regression}
\myindex{logistic regression}%

Logistic regression is available using the \function{glm()} function, 
which supports
a variety of
link functions and distributional forms for generalized linear models, including logistic regression.
\FoodForThought{The \function{glm()} function has argument \option{family}, which can take an option 
\option{link}.  The \code{logit} link is the default link for the binomial family,
so we don't need to specify it here. The more verbose usage would be \code{family=binomial(link=logit)}.}%
\Rindex{glm()}%
\Rindex{family option}%
\Rindex{exp()}%
<<help-logit,tidy=FALSE>>=
logitmod <- glm(homeless ~ age + female, family=binomial, 
  data=HELPrct)
summary(logitmod)
exp(coef(logitmod))
exp(confint(logitmod))
@

We can compare two models (for multiple degree of freedom tests).  For example, we
might be interested in the association of homeless status and age for each of the three substance groups.
\Rindex{anova()}%
\Rindex{test option}%
<<tidy=FALSE>>=
mymod1 = glm((homeless=="homeless") ~ age + substance, family=binomial, 
  data=HELPrct)
mymod2 = glm((homeless=="homeless") ~ age, family=binomial, 
  data=HELPrct)
summary(mymod1)
exp(coef(mymod1))
anova(mymod2, mymod1, test="Chisq")
@
We observe that the cocaine and heroin groups are significantly less likely to be homeless than alcohol involved subjects, after controlling for age.  (A similar result is seen when considering just homeless status and substance.)

<<>>=
tally(~ homeless | substance, format="percent", margins=TRUE, data=HELPrct)
@

\chapter{Survival Time Outcomes}

\myindex{survival analysis}%
\myindex{failure time analysis}%
\myindex{time to event analysis}%
Extensive support for survival (time to event) analysis is available within the 
\pkg{survival} package.

\section{Kaplan-Meier plot}

\myindex{Kaplan-Meier plot}%
\Rindex{survfit()}%
\Rindex{Surv()}%
\Rindex{conf.int option}%
\Rindex{xlab option}%
\begin{center}
<<help-km,fig.width=6,fig.height=3.9,tidy=FALSE,message=FALSE>>=
require(survival)
fit <- survfit(Surv(dayslink, linkstatus) ~ treat, 
  data=HELPrct)
plot(fit, conf.int=FALSE, lty=1:2, lwd=2, 
  xlab="time (in days)", ylab="P(not linked)")
legend(20, 0.4, legend=c("Control", "Treatment"), 
  lty=c(1,2), lwd=2)
title("Product-Limit Survival Estimates (time to linkage)")
@
\end{center}

We see that the subjects in the treatment (Health Evaluation and Linkage to Primary Care clinic) were significantly more likely to 
link to primary care (less likely to ``survive'') than the control (usual care) group.

\section{Cox proportional hazards model}
\myindex{Cox proportional hazards model}%
\myindex{proportional hazards model}%
\Rindex{coxph()}%

<<help-surv,tidy=FALSE>>=
require(survival)
summary(coxph(Surv(dayslink, linkstatus) ~ age + substance, 
  data=HELPrct))
@

Neither age, nor substance group was significantly associated with linkage to primary care.


\chapter{More than Two Variables}

\section{Two (or more) way ANOVA}

We can fit a two (or more) way ANOVA model, without or with an interaction,
using the same modeling syntax.
<<help-aovplot>>=
median(cesd ~ substance | sex, data=HELPrct)
bwplot(cesd ~ subgrp | sex, data=HELPrct)
@
<<help-aov2>>=
summary(aov(cesd ~ substance + sex, data=HELPrct))
@
<<help-aov3>>=
summary(aov(cesd ~ substance * sex, data=HELPrct))
@
There's little evidence for the interaction, though there are statistically
significant main effects terms for \variable{substance} group and 
\variable{sex}.

<<help-interaction,tidy=FALSE>>=
xyplot(cesd ~ substance, groups=sex, type='a',
  data=HELPrct)
@


\section{Multiple regression}
\myindex{multiple regression}%
\myindex{multivariate relationships}%

Multiple regression is a logical extension of the prior commands, where 
additional predictors are added.  This allows students to start to try to disentangle 
multivariate relationships.  

\InstructorNote{We tend to introduce multiple linear regression
early in our courses, as a purely descriptive technique, then return to it
regularly.  The motivation for this is described at length in the companion volume 
\emph{Start Modeling with R}.}

Here we consider a model (parallel slopes) for depressive symptoms as a function of Mental Component Score (MCS),
age (in years) and sex of the subject.
<<help-multreg>>=
lmnointeract <- lm(cesd ~ mcs + age + sex, data=HELPrct)
summary(lmnointeract)
@
\myindex{interactions}%
We can also fit a model that includes an interaction between MCS and sex.
<<help-multreg2>>=
lminteract <- lm(cesd ~ mcs + age + sex + mcs:sex, data=HELPrct)
summary(lminteract)
anova(lminteract)
@
<<help-multreg3>>=
anova(lmnointeract, lminteract)
@

There is little evidence for an interaction effect, so we drop
this from the model.

\subsection{Visualizing the results from the regression}

\label{sec:plotFun}
\Rindex{plotFun()}%
\Rindex{makeFun()}%
The \function{makeFun()} and \function{plotFun()} functions from the \pkg{mosaic} package 
can be used to display the results from a regression model.  For this example, we might 
display the predicted CESD values for a range of MCS values a 36 year old male and female subject from the parallel
slopes (no interaction) model.
<<makeFUN>>=
lmfunction = makeFun(lmnointeract)
@

\Rindex{xyplot()}%
\Rindex{auto.key option}%
\Rindex{ylab option}%
\Rindex{groups option}%
\Rindex{add option}%
We can now plot this function for male and female subjects over a range of MCS (mental component score) values, along
with the observed data for 36 year olds.  
<<plotFUN,tidy=FALSE,fig.keep='last'>>=
xyplot(cesd ~ mcs, groups=sex, auto.key=TRUE, 
  data=subset(HELPrct, age==36))
plotFun(lmfunction(mcs, age=36, sex="male") ~ mcs, 
  xlim=c(0,60), lwd=2, ylab="predicted CESD", add=TRUE)
plotFun(lmfunction(mcs, age=36, sex="female") ~ mcs, 
  xlim=c(0,60), lty=2, lwd=3, add=TRUE)
@


\subsection{Coefficient plots}

\myindex{coefficient plots}%
It is sometimes useful to display a plot of the coefficients for a multiple regression model (along with their associated
confidence intervals).

\Rindex{mplot()}%
<<>>=
mplot(lmnointeract, which=7, multiplot=FALSE)
@

\TeachingTip{Darker dots indicate regression coefficients where the 95\% confidence interval does not include the null hypothesis value of zero.}


\subsection{Residual diagnostics}
\myindex{residual diagnostics}
\myindex{regression diagnostics}

It's straightforward to undertake residual diagnostics for this model.  We begin by adding the
fitted values and residuals to the dataset.
\Caution{Be careful when fitting regression models with missing values (see also section \ref{sec:miss}).}
\TeachingTip{The \function{mplot} function can also be used to create these graphs.}
\Rindex{resid()}%
\Rindex{fitted()}%
\Rindex{abs()}%
<<>>=
HELPrct = transform(HELPrct, residuals = resid(lmnointeract))
HELPrct = transform(HELPrct, pred = fitted(lmnointeract))
@
<<tidy=FALSE>>=
histogram(~ residuals, xlab="residuals", fit="normal", 
  data=HELPrct)
@

We can identify the subset of observations with extremely large residuals.

\Rindex{abs()}%
<<>>=
subset(HELPrct, abs(residuals) > 25)
@

\Rindex{cex option}%
\Rindex{type option}%
<<tidy=FALSE>>=
xyplot(residuals ~ pred, ylab="residuals", cex=0.3,
  xlab="predicted values", main="predicted vs. residuals",
  type=c("p", "r", "smooth"), data=HELPrct)
@
<<tidy=FALSE>>=
xyplot(residuals ~ mcs, xlab="mental component score", 
  ylab="residuals", cex=0.3,
  type=c("p", "r", "smooth"), data=HELPrct)
@

The assumptions of normality, linearity and homoscedasticity seem reasonable here.
\begin{problem}
The \dataframe{RailTrail} dataset within the \pkg{mosaic} package includes the counts 
of crossings of a rail trail in Northampton, Massachusetts for 90 days in 2005.  
City officials are interested in understanding usage of the trail network, and
how it changes as a function of temperature and day of the week.  
Describe the distribution of the variable \variable{avgtemp} in terms of its
center, spread and shape.
<<tidy=FALSE>>=
favstats(~ avgtemp, data=RailTrail)
densityplot(~ avgtemp, xlab="Average daily temp (degrees F)", 
  data=RailTrail)
@
\end{problem}
\begin{solution}
The distribution of average temperature (in degrees Fahrenheit) is approximately normally
distributed with mean 57.4 degrees and standard deviation of 11.3 degrees.
\end{solution}
\begin{problem}
The \dataframe{RailTrail} dataset also includes a variable called \variable{cloudcover}.  
Describe the distribution of this variable in terms of its
center, spread and shape.
\end{problem}
\begin{solution}
<<>>=
favstats(~ cloudcover, data=RailTrail)
densityplot(~ cloudcover, data=RailTrail)
@
The distribution of cloud cover is ungainly (almost triangular), with increasing probability for more
cloudcover.  The mean is 5.8 oktas (out of 10), with standard deviation of 3.2 oktas.  It tends to be
cloudy in Northampton!
\end{solution}
\begin{problem}
The variable in the \dataframe{RailTrail} dataset that provides the daily count
of crossings is called \variable{volume}. 
Describe the distribution of this variable in terms of its
center, spread and shape.
\end{problem}
\begin{solution}
<<>>=
favstats(~ volume, data=RailTrail)
densityplot(~ volume, xlab="# of crossings", data=RailTrail)
subset(RailTrail, volume > 700)
@
The distribution of daily crossings is approximately normally
distributed with mean 375 crossings  and standard deviation of 127 crossings.
There is one outlier with 736 crossings which occurred on a Monday holiday in the spring
(Memorial Day).  
\end{solution}
\begin{problem}
The \dataframe{RailTrail} dataset also contains an indicator of whether the day was 
a weekday (\variable{weekday==1}) or a weekend/holiday (\variable{weekday==0}).
Use \function{tally()} to describe the distribution of this categorical variable.
What percentage of the days are weekends/holidays?
\end{problem}
\begin{solution}
<<>>=
tally(~ weekday, data=RailTrail)
tally(~ weekday, format="percent", data=RailTrail)
@
Just over 30\% of the days are weekends or holidays.
\end{solution}
\begin{problem}
Use side-by-side boxplots to compare the distribution of \variable{volume} by day type in the \dataframe{RailTrail} dataset.
Hint: you'll need to turn the numeric \variable{weekday} variable into a factor variable using \function{as.factor()}.
What do you conclude?
\end{problem}
\begin{solution}
<<>>=
bwplot(volume ~ as.factor(weekday), data=RailTrail)
@
or
<<>>=
RailTrail = transform(RailTrail, daytype = ifelse(weekday==1, "weekday", "weekend/holiday"))
bwplot(volume ~ daytype, data=RailTrail)
@
We see that the weekend/holidays tend to have more users.
\end{solution}

\begin{problem}
Use overlapping densityplots to compare the distribution of \variable{volume} by day type in the 
\dataframe{RailTrail} dataset.
What do you conclude?
\end{problem}
\begin{solution}
<<>>=
densityplot(volume ~ weekday, auto.key=TRUE, data=RailTrail)
@
We see that the weekend/holidays tend to have more users.
\end{solution}
\begin{problem}
Create a scatterplot of \variable{volume} as a function of \variable{avgtemp} using the \dataframe{RailTrail} dataset, along with a regression line and scatterplot
smoother (lowess curve).  What do you observe about the relationship?  
\end{problem}
\begin{solution}
<<>>=
xyplot(volume ~ avgtemp, xlab="average temperature (degrees F)", 
  type=c("p", "r", "smooth"), lwd=2, data=RailTrail)
@
We see that there is a positive relationship between these two variables, but the association is 
somewhat nonlinear (which makes sense as we wouldn't continue to predict an increase in usage when the 
temperature becomes uncomfortably warm).  
\end{solution}
\begin{problem}
Using the \dataframe{RailTrail} dataset,
fit a multiple regression model for \variable{volume} as a function of \variable{cloudcover}, \variable{avgtemp}, 
\variable{weekday} and the interaction
between day type and average temperature.
Is there evidence to retain the interaction term at the $\alpha=0.05$ level?
\end{problem}
\begin{solution}
<<>>=
fm = lm(volume ~ cloudcover + avgtemp + weekday + avgtemp:weekday, data=RailTrail)
summary(fm)
@
The interaction between average temperature and day-type is statistically significant (p=0.016).  We 
interpret this as being a steeper slope (stronger association) on weekdays rather than weekends.
(Perhaps on weekends/holidays people will tend to head out on the trails irrespective of the weather?)
\end{solution}
\begin{problem}
Use \function{makeFun()} to calculate the predicted number of crossings on a weekday with average 
temperature 60 degrees and no clouds.  Verify this calculation using the coefficients from the 
model.
<<>>=
coef(fm)
@
\end{problem}
\begin{solution}
<<>>=
myfun = makeFun(fm)
myfun(cloudcover=0, avgtemp=60, weekday=1)
@
We expect just over 480 crossings on a day with these characteristics.
\end{solution}
\begin{problem}
Use \function{makeFun()} and \function{plotFun()} to display predicted values for the number of crossings
on weekdays and weekends/holidays for average temperatures between 30 and 80 degrees and a cloudy day 
(\variable{cloudcover=10}).  
\end{problem}
\begin{solution}
<<>>=
myfun = makeFun(fm)
xyplot(volume ~ avgtemp, data=RailTrail)
plotFun(myfun(cloudcover=10, avgtemp, weekday=0) ~ avgtemp, lwd=2, add=TRUE)
plotFun(myfun(cloudcover=10, avgtemp, weekday=1) ~ avgtemp, lty=2, lwd=3, add=TRUE)
@
We
interpret this as being a steeper slope (stronger association) on weekdays rather than weekends.
(Perhaps on weekends/holidays people will tend to head out on the trails irrespective of the weather?)
\end{solution}
\begin{problem}
Using the multiple regression model, generate a histogram (with overlaid normal 
density) to assess the normality of the residuals.  
\end{problem}
\begin{solution}
<<>>=
histogram(~ resid(fm), fit="normal")
@
The distribution is approximately normal.
\end{solution}
\begin{problem}
Using the same model generate a scatterplot of the residuals versus predicted values and comment
on the linearity of the model and assumption of equal variance.
\end{problem}
\begin{solution}
<<>>=
xyplot(resid(fm) ~ fitted(fm), type=c("p", "r", "smooth"))
@
The association is fairly linear, except in the tails.  There's some evidence that the variability
of the residuals increases with larger fitted values.
\end{solution}
\begin{problem}
Using the same model generate a scatterplot of the residuals versus average temperature and comment
on the linearity of the model and assumption of equal variance.
\end{problem}
\begin{solution}
<<>>=
xyplot(resid(fm) ~ avgtemp, type=c("p", "r", "smooth"), data=RailTrail)
@
The association is somewhat non-linear.  There's some evidence that the variability
of the residuals increases with larger fitted values.
\end{solution}

\chapter{Probability Distributions and Random Variables}

\label{sec:DiscreteDistributions}
\label{sec:probability}
\myindex{random variables}%

\R\ can calculate quantities related to probability distributions of all types.  
It is straightforward to generate
random samples from these distributions, which can be used 
for simulation and exploration.
<<probdist,fig.height=4,fig.width=6>>=
xpnorm(1.96, mean=0, sd=1)    # P(Z < 1.96)
@
\Rindex{qnorm()}%
\Rindex{dnorm()}%
\Rindex{pnorm()}%
\Rindex{xpnorm()}%
\Rindex{rnorm()}%
\Rindex{integrate()}%
<<>>=
# value which satisfies P(Z < z) = 0.975
qnorm(.975, mean=0, sd=1)   
integrate(dnorm, -Inf, 0)   # P(Z < 0)
@
The following table displays the basenames for probability distributions 
available within base \R.  These functions can be prefixed by {\tt d} to 
find the density function for the distribution, {\tt p} to find the 
cumulative distribution function, {\tt q} to find quantiles, and {\tt r} to 
generate random draws. For example, to find the density function of an exponential
random variable, use the command \function{dexp()}.
The \function{qDIST()} function is the inverse of the 
\function{pDIST()} function, for a given basename {\tt DIST}. 
\begin{center}
\begin{tabular}{|c|c|c|} \hline
Distribution   & Basename                 \\ \hline
Beta           &  {\tt beta}        \\
binomial       &  {\tt binom}    \\
Cauchy         &  {\tt cauchy}   \\
chi-square     &  {\tt chisq}    \\
exponential    &  {\tt exp}      \\
F              &  {\tt f}        \\
gamma          &  {\tt gamma}    \\
geometric      &  {\tt geom}     \\
hypergeometric &  {\tt hyper}    \\
logistic       &  {\tt logis}    \\
lognormal      &  {\tt lnorm}    \\
negative binomial &  {\tt nbinom} \\
normal         &  {\tt norm}      \\
Poisson        &  {\tt pois}      \\
Student's t    &  {\tt t}        \\
Uniform        &  {\tt unif}     \\
Weibull        &  {\tt weibull}   \\ \hline
\end{tabular}
\end{center}
\DiggingDeeper{The \function{fitdistr()} within the \pkg{MASS} package facilitates estimation
of parameters for many distributions.}
The \function{plotDist()} can be used to display distributions in a variety of ways.
<<tidy=FALSE>>=
plotDist('norm', mean=100, sd=10, kind='cdf')
@
<<tidy=FALSE>>=
plotDist('exp', kind='histogram', xlab="x")
@
<<tidy=FALSE>>=
plotDist('binom', size=25, prob=0.25, xlim=c(-1,26))
@
\begin{problem}
Generate a sample of 1000 exponential random variables with rate parameter
equal to 2, and calculate the mean of those variables.
\end{problem}
\begin{solution}
<<expprob>>=
x <- rexp(1000, rate=2)
mean(x)
@
\end{solution}

\begin{problem}
Find the median of the random variable X, if it is exponentially distributed
with rate parameter 10.
\end{problem}
\begin{solution}
<<expprob2>>=
qexp(.5, rate=10)
@
\end{solution}


\chapter{Power Calculations}
\label{chap:onesamppower}

While not generally a major topic in introductory courses, power and sample size calculations
help to reinforce key ideas in statistics.  In this section, we will explore how \R\ can 
be used to undertake power calculations using analytic approaches.
We consider a simple problem with two tests (t-test and
sign test) of
a one-sided comparison.

We will compare the power of the sign test and the power of the test based on normal theory (one sample one sided t-test) assuming that $\sigma$ 
is known.
Let $X_1, ..., X_{25}$ be i.i.d. $N(0.3, 1)$ (this is the alternate that we wish to calculate power for).  Consider testing the null hypothesis $H_0: \mu=0$ versus $H_A: \mu>0$ at significance level $\alpha=.05$.  

\section{Sign test}

We start by calculating the Type I error rate for the sign test.  Here we want to
reject when the number of positive values is large.  Under the null hypothesis, this is
distributed as a Binomial random variable with n=25 trials and p=0.5 probability of being
a positive value.  Let's consider values between 15 and 19.

<<pbinom>>=
xvals <- 15:19
probs <- 1 - pbinom(xvals, size=25, prob=0.5)
cbind(xvals, probs)
qbinom(.95, size=25, prob=0.5)
@
So we see that if we decide to reject when the number of positive values is
17 or larger, we will have an $\alpha$ level of \Sexpr{round(1-pbinom(16, 25, 0.5), 3)},
which is near the nominal value in the problem.

We calculate the power of the sign test as follows. The probability that $X_i > 0$, given that $H_A$ is true is given by:
<<pnorm1>>=
1 - pnorm(0, mean=0.3, sd=1)
@
We can view this graphically using the command:
\begin{center}
<<pnorm2,fig.width=5,fig.height=1.9>>= 
xpnorm(0, mean=0.3, sd=1, lower.tail=FALSE)
@
\end{center}
The power under the alternative is equal to the probability of getting 17 or more positive values,
given that $p=0.6179$:

\Rindex{pbinom()}%
<<pbinom2>>=
1 - pbinom(16, size=25, prob=0.6179)
@
The power is modest at best.

\section{T-test}

We next calculate the power of the test based on normal theory.  To keep the comparison
fair, we will set our $\alpha$ level equal to 0.05388.

<<tidy=FALSE>>=
alpha <- 1-pbinom(16, size=25, prob=0.5); alpha
@

First we find the rejection region.  
<<tidy=FALSE>>=
n <- 25; sigma <- 1 # given
stderr <- sigma/sqrt(n)
zstar <- qnorm(1-alpha, mean=0, sd=1)
zstar
crit <- zstar*stderr
crit
@


\noindent
Therefore, we reject for observed means greater than \Sexpr{round(crit,3)}.  

To calculate the power of this one-sided test we find the probability
under the alternative hypothesis 
to the right of this cutoff.

<<>>=
power <- 1 - pnorm(crit, mean=0.3, sd=stderr)
power
@

The power of the test based on normal theory is \Sexpr{round(power,3)}.
To provide a check (or for future calculations of this sort) we can use the 
\function{power.t.test()} function.
<<>>=
power.t.test(n=25, delta=.3, sd=1, sig.level=alpha, alternative="one.sided",
type="one.sample")$power
@

This analytic (formula-based approach) yields a similar estimate to the value that we calculated directly.  

Overall, we see that the t-test has higher power than the sign test, if the underlying
data are truly normal.  \TeachingTip{It's useful to have students calculate power empirically, 
to demonstrate the power of simulations.}
\begin{problem}
\label{prob:power1}%
Find the power of a two-sided two-sample t-test where both distributions 
are approximately normally distributed with the same standard deviation, but the group differ by 50\% of the standard deviation.  Assume that there are 
\Sexpr{n}
observations per group and an alpha level of \Sexpr{alpha}.
\end{problem}
\begin{solution}
<<setup1,echo=FALSE>>=
n <- 100
alpha <- 0.01
@
<<power>>=
n
alpha
power.t.test(n=n, delta=.5, sd=1, sig.level=alpha)
@
\end{solution}
\begin{problem}
Find the sample size needed to have 90\% power for a two group t-test
where the true 
difference between means is 25\% of the standard deviation in the groups
(with $\alpha=0.05$).
\end{problem}
\begin{solution}
<<power2>>=
power.t.test(delta=.25, sd=1, sig.level=alpha, power=0.90)
@
\end{solution}


\chapter{Data Management}
\label{sec:manipulatingData}%
\myindex{data management}%
\myindex{thinking with data}%

Data management is a key capacity to allow students (and instructors) to ``compute with data'' or  
as Diane Lambert of Google has stated, ``think with data''.
We tend to keep student data management to a minimum during the early part of an introductory
statistics course, then gradually introduce topics as needed.  For courses where students
undertake substantive projects, data management is more important.  This chapter describes 
some key data management tasks.

\section{Adding new variables to a data frame}
\myindex{data frame}%
We can add additional variables to an existing data frame (name for a dataset in \R) by simple assignment.

<<mr-adding-variable2>>=
head(iris)
@

\myindex{adding variables}%
\Rindex{transform()}%
\Rindex{cut()}%
<<mr-adding-variable>>=
# cut places data into bins
iris <- transform(iris, Length = cut(Sepal.Length, 4:8))    
@

<<"mr-adding-variable2-again">>=
head(iris)
@
\Rindex{head()}%
\myindex{display first few rows}%

\myindex{CPS85 dataset}%
The \dataframe{CPS85} data frame contains data from a Current Population Survey (current in 1985, that is).
Two of the variables in this data frame are \variable{age} and \variable{educ}.  We can estimate
the number of years a worker has been in the workforce if we assume they have been in the workforce
since completing their education and that their age at graduation is 6 more than the number
of years of education obtained.  We can this as a new variable in the data frame simply
by assigning to it:
\myindex{CPS85 dataset}%
<<>>=
CPS85 <- transform(CPS85, workforce.years = age - 6 - educ)
favstats(~ workforce.years, data=CPS85)
@
In fact this is what was done for all but one of the cases to create the \variable{exper} 
variable that is already in the \dataframe{CPS85} data.
<<>>=
tally(~ (exper - workforce.years), data=CPS85)
@

\section{Dropping variables}
\myindex{dropping variables}%
\Rindex{subset()}%
\Rindex{select option}%
Since we already have \variable{educ}, there is no reason to keep our new variable.  Let's drop it.
Notice the clever use of the minus sign.
<<>>=
names(CPS85)
CPS1 <- subset(CPS85, select = -workforce.years)
names(CPS1)
@

Any number of variables can be dropped or kept in this manner by supplying a vector
of variable names.
<<>>=
CPS1 <- subset(CPS85, select = -c(workforce.years, exper))
@

If we only want to work with the first few variables, we can discard the rest in a similar way.
Columns can be specified by number as well as name (but this can be dangerous if you are wrong 
about where the columns are):
<<>>=
CPSsmall <- subset(CPS85, select=1:4)
head(CPSsmall,2)
@

\section{Renaming variables}
\myindex{renaming variables}%
\Rindex{row.names()}%
Both the column (variable) names and the row names of a data frames can be changed by
simple assignment using \function{names()} or \function{row.names()}.
<<>>=
ddd <- data.frame(number=1:5, letter=letters[1:5])
row.names(ddd) <- c("Abe","Betty","Claire","Don","Ethel")
ddd                        # row.names affects how a data.frame prints
@
More interestingly, it is possible to reset just individual names with the following
syntax.
<<>>=
# misspelled a name, let's fix it
row.names(ddd)[2] <- "Bette"         
row.names(ddd)
@

\Rindex{names()}%
\myindex{faithful dataset}%
The \dataframe{faithful} data set (in the \pkg{datasets} package, which is always available)
has very unfortunate names.
\TeachingTip{It's a good idea to start teach good practices for choice of variable names from day one.}
<<>>=
names(faithful)
@
The measurements are the duration of an euption and the time until the subsequent eruption,
so let's give it some better names.
<<>>=
names(faithful) <- c('duration', 'time.til.next')
head(faithful, 3)
@
\myindex{faithful dataset}%
\begin{center}
<<"mr-faithful-xy">>=
xyplot(time.til.next ~ duration, alpha=0.5, data=faithful)
@
\end{center}
If the variable containing a data frame is modified or used to store a different object,
the original data from the package can be recovered using \function{data()}.
\Rindex{data()}%
<<>>=
data(faithful)
head(faithful, 3)
@

\begin{problem}
Using \dataframe{faithful} data frame, make a scatter plot of eruption duration times vs. the time
since the previous eruption.
\end{problem}

If we want to rename a variable, we can do this using \function{names()}.
For example, perhaps we want to rename \variable{educ} (the second column) to \variable{education}.
<<>>=
names(CPS85)[2] <- 'education'
CPS85[1,1:4]
@

If we don't know the column number (or generally to make our code clearer), a few more 
keystrokes produces:
<<>>=
names(CPS85)[names(CPS85) == 'education'] <- 'educ'
CPS85[1,1:4]
@

\section{Creating subsets}
\myindex{creating subsets}%
\myindex{subsets of data frames}%
\label{sec:subsets}
We can also use \function{subset()} to reduce the size of a data frame by selecting 
only certain rows.
\begin{center}
<<"mr-faithful-long-xy">>=
data(faithful)
names(faithful) <- c('duration', 'time.til.next')
# any logical can be used to create subsets
faithfulLong <- subset(faithful, duration > 3)        
xyplot( time.til.next ~ duration, data=faithfulLong )
@
\end{center}

Of course, if all we want to do is produce a graph, there is no reason to create 
a new data frame.  The plot above could also be made with:
\Caution{Unfortunately, not all functions in R (e.g., \function{stem()}) support the \function{subset=} or
\option{data=} options.}
<<eval=FALSE,tidy=FALSE>>=
xyplot(time.til.next ~ duration, subset=duration > 3, 
  data=faithful)
@



\section{Sorting data frames}
\myindex{sorting data frames}%
\Rindex{order()}%

Data frames can be sorted using the \function{order()} function.
<<>>=
head(faithful, 3)
sorted = faithful[order(faithful$duration),]
head(sorted, 3)
@



\section{Merging datasets}
\myindex{merging data frames}%

The \dataframe{fusion1} data frame in the \pkg{fastR} package contains 
genotype information for a SNP (single nucleotide polymorphism) in the gene
\emph{TCF7L2}.  
The \dataframe{pheno} data frame contains phenotypes
(including type 2 diabetes case/control status) for an intersecting set of individuals.
We can merge these together to explore the association between
genotypes and phenotypes using \verb!merge()!.

<<>>=
require(fastR)
fusion1 = fusion1[order(fusion1$id),]
head(fusion1,3)
head(pheno,3)
@

\Rindex{merge()}%
\Rindex{all.x option}%
\Rindex{by.x option}%
<<tidy=FALSE>>=
# merge fusion1 and pheno keeping only id's that are in both
fusion1m <- merge(fusion1, pheno, by.x='id', by.y='id', 
  all.x=FALSE, all.y=FALSE)
head(fusion1m, 3)
@
In this case, since the values are the same for each data frame, we could collapse
\option{by.x} and \option{by.y} to \option{by} and collapse
\option{all.x} and \option{all.y} to \option{all}.
The first of these specifies which column(s) to use to identify matching cases.
The second indicates whether cases in one data frame that do not appear in the other 
should be kept (\code{TRUE}) or dropped 
(filling in \code{NA} as needed) or dropped from the merged data frame.

\myindex{fusion1 dataset}%
Now we are ready to begin our analysis.
<<"mr-fusion1-xtabs">>=
tally(~t2d + genotype, data=fusion1m)
@

\begin{problem}
The \dataframe{fusion2} data set in the \pkg{fastR} package contains genotypes for 
another SNP.  Merge \dataframe{fusion1}, \dataframe{fusion2}, and \dataframe{pheno} into a single data
frame.

Note that \dataframe{fusion1} and \dataframe{fusion2} have the same columns.
<<>>=
names(fusion1)
names(fusion2)
@
You may want to use the \option{suffixes} argument to \function{merge()} or rename the variables
after you are done merging to make the resulting data frame easier to navigate.

Tidy up your data frame by dropping any columns that are redundant or that you just don't want to
have in your final data frame.
\end{problem}

\section{Slicing and dicing}
\myindex{reshaping data frames}%
\myindex{transforming data frames}%
\myindex{transposing data frames}%

The \function{reshape()} function provides a flexible way to change the arrangement of data.  
\Rindex{reshape()}%
It was designed for converting between long and wide versions of 
time series data and its arguments are named with that in mind.

A common situation is when we want to convert from a wide form to a 
long form because of a change in perspective about what a unit of 
observation is.  For example, in the \dataframe{traffic} data frame, each 
row is a year, and data for multiple states are provided.

<<"mr-traffic-reshape">>=
traffic
@
We can reformat this so that each row contains a measurement for a 
single state in one year.

\Rindex{reshape()}%
\Rindex{varying option}%
<<tidy=FALSE>>=
longTraffic <-
  reshape(traffic[,-2], idvar="year", 
  ids=row.names(traffic),
  times=names(traffic)[3:6], 
  timevar="state",
  varying=list(names(traffic)[3:6]),
  v.names="deathRate",
  direction="long") 
head(longTraffic)
@
And now we can reformat the other way, this time having all data for a given state 
form a row in the data frame.
<<tidy=FALSE>>=
stateTraffic <- reshape(longTraffic, direction='wide', 
  v.names="deathRate", idvar="state", timevar="year")
stateTraffic
@

In simpler cases, \function{stack()} or \function{unstack()} may suffice.
The \pkg{Hmisc} package also provides \function{reShape()} as an alternative 
to \function{reshape()}.
\Rindex{stack()}%
\Rindex{unstack()}%

\section{Derived variable creation}
\myindex{derived variables}

A number of functions help facilitate the creation or recoding of variables.

\subsection{Creating categorical variable from a quantitative variable}

Next we demonstrate how to 
create a three-level categorical variable
with cuts at 20 and 40 for the CESD scale (which ranges from 0 to 60 points).

\Rindex{cut()}%
\Rindex{include.lowest option}%
\Rindex{breaks option}%
<<tidy=FALSE>>=
favstats(~ cesd, data=HELPrct)
HELPrct = transform(HELPrct, cesdcut = cut(cesd, 
  breaks=c(0, 20, 40, 60), include.lowest=TRUE))
bwplot(cesd ~ cesdcut, data=HELPrct)
@
\Rindex{ntiles()}%
\TeachingTip{The \function{ntiles} function can be used to automate creation of groups in this manner.}

It might be preferable to give better labels.
<<tidy=FALSE>>=
HELPrct = transform(HELPrct, cesdcut = cut(cesd, 
  labels=c("low", "medium", "high"),
  breaks=c(0, 20, 40, 60), include.lowest=TRUE))
bwplot(cesd ~ cesdcut, data=HELPrct)
@


\subsection{Reordering factors}
\myindex{reordering factors}%
\myindex{factor reordering}%
\Rindex{relevel()}%
\Rindex{transform()}%
\Rindex{coef()}%
\Rindex{tally()}%
By default R uses the first level in lexicographic order as the reference group for modeling.  This 
can be overriden using the \function{relevel()} function (see also \function{reorder()}).
<<tidy=FALSE>>=
tally(~ substance, data=HELPrct)
coef(lm(cesd ~ substance, data=HELPrct))
HELPrct = transform(HELPrct, subnew = relevel(substance, 
  ref="heroin"))
coef(lm(cesd ~ subnew, data=HELPrct))
@
\Caution{It is usually better to make new datasets rather than modifying the original.}

\section{Accounting for missing data}
\label{sec:miss}

\myindex{missing data}%
\myindex{incomplete data}%
\Rindex{subset()}%
\Rindex{dim()}%
\Rindex{select option}%
\Rindex{NA character}%
Missing values arise in almost all real world investigations.  R uses the \variable{NA} character as an 
indicator for missing data.  The \dataframe{HELPmiss} dataframe within the \pkg{mosaic} package includes all 
$n=470$ subjects enrolled at baseline (including the $n=17$ subjects with some missing data who 
were not included in \dataframe{HELPrct}).  
\myindex{HELPmiss dataset}%
<<tidy=FALSE>>=
smaller = subset(HELPmiss, select=c("cesd", "drugrisk", 
  "indtot", "mcs", "pcs", "substance"))
dim(smaller)
summary(smaller)
@

Of the 470 subjects in the 6 variable data frame, only the \code{drugrisk}, \code{indtot}, \code{mcs}, and \code{pcs} variables have missing values.  

\Rindex{with()}%
\Rindex{na.omit()}%
\Rindex{favstats()}%
<<tidy=FALSE>>=
favstats(~ mcs, data=smaller)
with(smaller, sum(is.na(mcs)))
nomiss = na.omit(smaller)
dim(nomiss)
favstats(~ mcs, data=nomiss)
@

\chapter{Health Evaluation and Linkage to Primary Care (HELP) Study}

\label{sec:help}

\myindex{HELP study}%
\myindex{Health Evaluation and Linkage to Primary Care study}%
Many of the examples in this guide utilize data from the HELP study,
a randomized clinical trial for adult inpatients recruited from a detoxification unit.
Patients with no primary care physician were randomized to receive a multidisciplinary assessment and a brief motivational intervention or usual care,
with the goal of linking them to primary medical care.
Funding for the HELP study was provided by the National Institute
on Alcohol Abuse and Alcoholism (R01-AA10870, Samet PI) and
National Institute on Drug Abuse (R01-DA10019, Samet PI).
The details of the
randomized trial along with the results from a series of additional analyses have been published\cite{same:lars:hort:2003,lieb:save:2002,kert:hort:frie:2003}.

Eligible subjects were
adults, who spoke Spanish or English, reported alcohol, heroin or
cocaine as their first or second drug of choice, resided in proximity
to the primary care clinic to which they would be referred or were
homeless.  Patients with established primary care relationships
they planned to continue, significant dementia, specific plans to
leave the Boston area that would prevent research participation,
failure to provide contact information for tracking purposes, or
pregnancy were excluded.

Subjects were interviewed at baseline during
their detoxification stay and follow-up interviews were undertaken
every 6 months for 2 years.  A variety of continuous, count, discrete, and survival time predictors and outcomes were collected at each of these five occasions.
The Institutional Review Board of
Boston University Medical Center approved all aspects of the study, including the creation of the de-identified dataset.  Additional
privacy protection was secured by the issuance of a Certificate of
Confidentiality by the Department of Health and Human Services.

The \pkg{mosaic} package contains several forms of the de-identified HELP dataset.
We will focus on \pkg{HELPrct}, which contains
27 variables for the 453 subjects
with minimal missing data, primarily at baseline.
Variables included in the HELP dataset are described in Table \ref{tab:helpvars}.  More information can be found here\cite{Horton:2011:R}.
A copy of the study instruments can be found at: \url{http://www.amherst.edu/~nhorton/help}.
\begin{longtable}{|p{2.1cm}|p{6.8cm}|p{3.5cm}|}
\caption{Annotated description of variables in the \dataframe{HELPrct} dataset}
\label{tab:helpvars} \\
\hline
VARIABLE & DESCRIPTION (VALUES) & NOTE \\ \hline
\variable{age} & age at baseline (in years) (range 19--60) & \\ \hline
\variable{anysub} & use of any substance post-detox & see also \variable{daysanysub}
\\ \hline
\variable{cesd} & Center for Epidemiologic Studies Depression scale (range 0--60, higher scores indicate more depressive symptoms)  & \\ \hline
\variable{d1} & how many times hospitalized for medical problems (lifetime)  (range 0--100)  & \\ \hline
\variable{daysanysub} & time (in days) to first use of any substance post-detox (range 0--268)  & see also \variable{anysubstatus} \\ \hline
\variable{dayslink} & time (in days) to linkage to primary care (range 0--456)  & see also \variable{linkstatus}
\\ \hline
\variable{drugrisk} & Risk-Assessment Battery (RAB) drug risk score  (range 0--21)  & see also \variable{sexrisk}
\\ \hline
\variable{e2b} & number of times in past 6 months entered a detox program  (range 1--21)  & \\ \hline
\variable{female} & gender of respondent  (0=male, 1=female)  &
\\ \hline
\variable{g1b} & experienced serious thoughts of suicide (last 30 days, values 0=no, 1=yes)  &
\\ \hline
\variable{homeless} & 1 or more nights on the street or shelter in past 6 months (0=no, 1=yes) & 
\\ \hline
\variable{i1} & average number of drinks (standard units) consumed per day (in the past 30 days, range 0--142) & see also \variable{i2}
\\ \hline
\variable{i2} & maximum number of drinks (standard units) consumed per day (in the past 30 days range 0--184) & see also \variable{i1}
\\ \hline
\variable{id} & random subject identifier (range 1--470) &
\\ \hline
\variable{indtot} & Inventory of Drug Use Consequences (InDUC) total score  (range 4--45)  &
\\ \hline
\variable{linkstatus} & post-detox linkage to primary care (0=no, 1=yes)  & see also \variable{dayslink}
\\ \hline
\variable{mcs} & SF-36 Mental Component Score  (range 7-62, higher scores are better)  & see also \variable{pcs}
\\ \hline
\variable{pcs} & SF-36 Physical Component Score  (range 14-75, higher scores are better)  & see also \variable{mcs}
\\ \hline
\variable{pss\_fr} & perceived social supports (friends, range 0--14) & 
\\ \hline
\variable{racegrp} & race/ethnicity (black, white, hispanic or other)  &  \\ \hline
\variable{satreat} & any BSAS substance abuse treatment at baseline (0=no, 1=yes)  &  \\ \hline
\variable{sex} & sex of respondent  (male or female)  & \\ \hline
\variable{sexrisk} & Risk-Assessment Battery (RAB) sex risk score  (range 0--21)  & see also \variable{drugrisk}
\\ \hline
\variable{substance} & primary substance of abuse (alcohol, cocaine or heroin) &
\\ \hline
\variable{treat} & randomization group (randomize to HELP clinic, no or yes) & 
\\ \hline
\end{longtable}
\noindent
Notes: Observed range is provided (at baseline) for continuous variables.


\chapter{Exercises and Problems}

\shipoutProblems

\bibliographystyle{alpha}
\bibliography{../include/USCOTS}
