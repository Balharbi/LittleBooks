
<<echo=FALSE,include=FALSE>>=
source('../include/setup.R')
opts_chunk$set( fig.path="figures/Multivariable-" ) 
if (!exists("..makingMaster..")) {
	set_parent('Master-Modeling.Rnw')
}
set.seed(123)
@



\chapter*{Preface}

These notes present a strategy for teaching statistical modeling. The strategy is based around notation: a way of writing down forms of relationships among variables.  Coupled with modern computing, the notation comes to life.  The form of a relationship is translated by the computer into a fit that describes the data at hand.

The same notation that can describe simple relationships --- for example, groupwise means, in which a quantitative response variable is averaged separately in groups defined by a categorical explanatory variable --- can be extended to much richer relationships involving multiple explanatory variables.  What's key is to have a notation in which phrases like ``response variable'' and ``explanatory variable'' have a discernible identity.  Starting with that notation puts students in a good place on the road to learning about modeling.

The notes are part of a series on teaching with the R computer environment, but they are not primarily about R.  Other notes in the series introduce R, discuss how to teach with R, and show how to carry out basic processes of statistical inference using conceptually simple operations implemented transparently in R.

If you don't already know R, we hope that you will think the commands are simple enough that you can use them yourself; that you can learn R by observation.  Our students appear to be able to do that.  

It would be possible to implement the strategy presented here using other software.  It might even be possible to present it without using software at all.  But you always need some notation to communicate. The R computer notation is simple and concise, a rival for traditional mathematical notation, even as it extends to include operations not in the algebric repertoire: sampling, randomization, iteration, etc. 

These notes are meant for teachers; this isn't a textbook for students.  Examples of materials for students, including a textbook \cite{stat-modeling-fresh-approach}, are available at \url{www.mosaic-web.org/StatisticalModeling}.



<<include=FALSE>>=
require(mosaic)
require(fastR)
options(na.rm=TRUE)
@


\chapter{Graphics \& Formulas}

\marginnote{\texttt{require(mosaic)}\\
The R system provides a way for the software development community to add new functionality.  "Packages" are the standard way to deliver such software.  
The statement \texttt{require(mosaic)} loads in the \pkg{mosaic} package, which itself includes several other packages.  

The \pkg{mosaic} package provides many of the commands that are used in this book.

Once the package is loaded, it remains loaded for the rest of the R session.  

As you teach, you may find other packages, such as \pkg{fastR}, that give useful data sets or other capabilities.  These can be loaded in the same way, for example:
<<>>=
require(fastR)
@
}



\marginnote{The \texttt{trebuchet} data set was collected by a high-school student, Andrew Pruim, as part of a Science Olympiad competition.  These data are included in the \pkg{fastR} package.  

A trebuchet is a device used for throwing projectiles. A heavy counter-weight pulls down the short end of the arm, rapidly accelerating a projectile hanging in a sack at the long end of the arm.  Introduced in medieval times, trebuchets were used as a siege weapons to destroy fortifications. 

\includegraphics[width=2in]{images/Trebuchet2.png}}

It's worthwhile to start a statistics course with graphics:  scatterplots, box-and-whisker plots, histograms, etc.  In addition to conveying well ideas of variation, graphics are close to the data themselves, compelling, and motivating.  Students see the advantage of using software; they couldn't make such graphics by hand.

\authNote{I'm simplifying the trebuchet data a bit, considering only \texttt{form=='a'}.
<<>>=
trebuchet = droplevels(subset(trebuchet, form=="a", drop=TRUE))
@
}
 

In R, these basic graphics can be made with \function{xyplot} and \function{bwplot}, for instance:
<<ex1,fig.width=4,fig.height=2,fig.keep='all'>>=
xyplot(distance ~ projectileWt,data=trebuchet)
bwplot(distance ~ object,data=trebuchet)
@

Each of these examples involves two variables.  They are often called $y$ and $x$ by students.  Alternative names frequently encountered are ``vertical axis" and ``horizontal axis,'', ``dependent'' and ``independent'' variables, ``input'' and ``output.''  Among math teachers, ``ordinate'' and ``abscissa'' are sometimes the preferred terms.  Whatever you choose to call them --- we'll use \term{explanatory} and \term{response} variables when describing models, and \term{input} and \term{output} for functions --- the notation must make clear which variable is in which role.  

In R notation, a \term{formula} is an expression involving the \verb+~+ squiggle character --- ``tilde'' --- that provides slots for laying out how you want to relate variables: y versus x, what to break down by what.  

There's more going on here than just identifying which variable gets plotted on each axis: the formula in R provides a {\em modeling language} that gives an easy path from basic graphics and basic statistics to multivariable modeling.  The path is easy enough that it makes sense to start in that direction early: using the concepts, terminology, and techniques of modeling as a way of introducing statistics.  Here is what such a journey might look like.

\section{Formulas \& Basic Statistics}




\marginnote{It's tempting to show students shortcuts for looking at a data set, such as \function{summary}.
<<eval=FALSE>>=
summary(trebuchet)
@
This unfortunately encourages students to think that there is such a thing  as a mean or median of a dataset.  It's important to distinguish between variables and the datasets in which variables are contained.}

The move to quantitative statistics is straightforward, since the syntax is very much the same as with plots.  For instance
<<>>=
mean(distance,data=trebuchet)
sd(distance,data=trebuchet)
@
In each case, the variable of interest is named along with the data set in which that variable is included.  

Using \pkg{mosaic}, the formula interface works with these basic statistics in much the same way as for graphics, e.g:
<<>>=
mean(distance~object,data=trebuchet)
@

Eventually, the formula will be used to guide students to thinking about using explanatory variables to account for a response variable, in this example, explaining distance by object.  At this point, it works well to present this to students as ``breaking down'' the distance variable by the object variable, or just ``dividing up into groups.''  

\chapter{From Means to Models}

Students often understand means algorithmically in a way that can be expressed in plain English: ``add them up and divide by $n$.''  Such arithmetic is considered basic.

Techniques such as regression involve more complicated algorithms, ones that require algebraic notation.  But strip away the algorithm used to compute the parameters of the regression and consider just the statement of the regression operation itself.  Here is such a statement in R, modeling distance flown by the projectile as a function of the projectile's weight:
<<>>=
mod1 = lm(distance~projectileWt,data=trebuchet)
@
The syntax for constructing the model is very much the same as for \function{mean}.  But what is the output, what is the model itself?

It helps to have some vocabulary:
\begin{itemize}
\item A model of this sort links inputs, called ``explanatory variables," to an output, or ``response variable". \sidenote{Depending on the field, these may be called dependent and independent variables respectively.} In the example above, \VN{distance} is the response variable while \VN{projectileWt} an the explanatory variable.
\item The model corresponds to a function, a mathematical representation of a relationship between an input and an output. 
\item The purpose of \function{lm} is to construct a function that is as close to the data as possible.  That is, \function{lm} ``fits'' the model to the data.
\end{itemize}

Students typically study functions using a traditional algebraic notation, e.g. 
$$ y = 4 x + 2 .$$
Such notation doesn't emphasize the idea that $y$ is a function of $x$, that $x$ is the input and $y$ is the output. Indeed, there's nothing explicit to say that $y$ is a function at all; usually it's understood to be a variable.

To introduce students to a notation that makes such relationships explicit, \pkg{mosaic} provides \function{makeFun}:
<<>>=
y = makeFun(4*x + 2 ~ x)
@

You can evaluate the function by specifying the inputs, for instance:
<<>>=
y(3)
@

Of course, functions such as this can be plotted in the ordinary way:
<<>>=
plotFun( y(x)~x, x.lim=range(-2,2))
@

The model, \texttt{mod1}, created earlier, is not exactly a function.\marginnote{You can see what's in \texttt{mod1} by giving it as a command, 
<<eval=FALSE>>=
mod1
@
This displays the model coefficients.  In addition to \function{makeFun}, you can use such operations as \function{r.squared}, \function{fitted}, and \function{resid}.} It's set up this way because there are several different types of information one might want to get from a model, not just the model function.  You can use \function{makeFun} to extract the model function from \texttt{mod1}: 
<<>>=
f1 = makeFun(mod1)
@
This use of \function{makeFun} reformats the information that's already in \texttt{mod1}  as a mathematical function in R. Once that's done, the function can be evaluated in the ordinary way, by specifying the inputs.

The function \texttt{f1} attempts to describe the relationship between the explanatory and response variables: between \VN{distance} and \VN{projectileWt}.


There are, of course, other possible functions that could be used to describe the relationship between \VN{distance} and \VN{projectileWt}.  Taking some measurements off the scatter plot, one might reasonably try a simple function, perhaps $y = 1900 - 30*x$.  No reason to use the names $y$ and $x$, however.  You can define the proposed function using the names in the data set:
<<>>=
f2 = makeFun( 1900 - 30*weight ~ weight)
@

Given these two functions, \function{f1} and \function{f2}, it's natural to ask, ``Which is which is better?''  Graphically, the answer is not so clear.  In the following plot, \function{f2} is drawn as the thicker line:
<<>>=
xyplot( distance ~ projectileWt, data=trebuchet)
plotFun(f1(projectileWt)~projectileWt, add=TRUE)
plotFun(f2(projectileWt)~projectileWt, add=TRUE, lwd=3)
@

The usual way to quantify how close each function comes to the data involves the residuals: the difference between the value as given by the model function and the actual data value.  For instance, for a \VN{projectileWt} of 60, the function values are:\marginnote{For models created fitted with \function{lm}, you can access the fitted values or residuals directly from the model structure, without needing to construct the function and evaluate it.  Use
<<eval=FALSE>>=
fitted(mod1)
resid(mod1)
@
}
<<>>=
f1(60)
f2(60)
@
You can be more systematic and evaluate the functions at every one of the data points:
<<tidy=FALSE>>=
trebuchet = transform(trebuchet,
                      f1resid=f1(projectileWt)-distance,
                      f2resid=f2(projectileWt)-distance)
@


\marginnote{NOTE IN DRAFT: Include \function{sum} in the aggregating statistics.}

There is one residual for each row in the dataset.  There are several good ways to describe the size of the residuals, e.g. the standard deviation, variance, or the sum of squares.  Here is the standard deviation.
<<>>
sd(f1resid,data=trebuchet)
sd(f2resid,data=trebuchet)
@
It appears that the residuals from \function{f1} are smaller.

It's a good exercise to try out many different alternatives.  You won't be able to find any whose sum of square residuals are smaller than those produced by \function{f1}.  In this sense, \function{f1}, from the model constructed by \function{lm}, is the best of all possible straight-line models of the data.

There are several ways to see the effect of \VN{projectileWt} on \VN{distance}.  The coefficients carry this information, but students need to be taught how to interpret them:
<<>>=
coef(mod1)
@
This says that for every 1 gm increase in the weight of the projectile, the distance flown decreases by 14.3 cm.  (The units of the variables are given in the help file for the data, `help(trebuchet)`.)

Another way to see this same thing, perhaps a bit more obvious to the introductory student, is to evaluate the model function at two different weights, for instance:
<<>>=
f1(51) - f1(50) 
@

It's perfectly reasonable at this point to consider the extent to which the data dictate the best fit, and whether other possible straight-line functions, \marginnote{Resampling provides an accessible way to explore the sampling variation of a model.  See \cite{resampling-book}.}even if their residuals are not as small as those from \function{f1}, are reasonable fits.  

For models created by \function{lm}, access to confidence intervals is provided by the model function.  Just ask for the interval, specifying whether you want an interval on the model value itself or on the predicted output for a given input.
<<>>=
f1(50,interval="confidence",level=0.95)
f1(50,interval="prediction",level=0.95)
@

Notice the absurdly wide prediction interval, which includes a non-physical negative distance, even though no backward-going launches are seen in the data itself.  This suggests that there's something wrong with the model for the purpose of making a prediction.  Let's go there.

\section{Multiple Inputs}

Consider why one might build a model of a trebuchet.  A practical application, if you are a medieval warrior, is to predict the distance travelled by a projectile: What weight is needed to reach the castle walls?

A competent trebuchet technician will tell you that there's another issue: How heavy is the counter-weight on the trebuchet?  You have the option of adding or taking away weight from the counter-weight in order to get your projectile on target.  

Fitting a relevant model is a matter of including the counter-weight (\VN{counterWt}) in the set of explanatory variables.  (It's also important that student Andrew Pruim collected the experimental data over a range of counter-weights.)  Here's a simple model:
<<tidy=FALSE>>=
mod2 = lm(distance~projectileWt+counterWt,data=trebuchet)
@

To extract the corresponding function, which will be a function of both \VN{projectileWt} and \VN{counterWt}, use \function{makeFun}.  We'll call the function \function{ballistic}:
<<>>=
ballistic = makeFun(mod2)
@

There are two input variables here, so an appropriate graphical display is a contour plot
<<fig.height=4,fig.width=4,tidy=FALSE>>=
plotFun(ballistic(projectileWt=x,counterWt=y)~x&y, 
        x.lim=range(10,70),y.lim=range(.5,3),
        xlab="Projectile (gms)",
        ylab="Counter Weight (kg)",
        main="Distance Thrown")
@

The first two lines of this do all the work, the remaining lines are just for setting labels.  Note that $x$ and $y$ are being used as the plotting variables.  $x$ is assigned to be the value of the projectile weight while $y$ is the value of the counter-weight.  It's helpful to use such explicit names for the input variables just to avoid accidentally reversing the meaning of the variables. With functions of more than one input it's easy to get things wrong.


It takes a bit of practice to learn to interpret such graphs, but it's worth the time.  Each contour shows the set of projectile weights and counter weights that can reach a given distance.  For instance, to reach 600 cm distance, the model suggests that one could use a projectile weight of 20 gm and a counterweight of about 1.25 kg, or one could up the projectile weight to 60 gm and increase the counterweight to a bit more than 2 kg.

Adding in the counter-weight as an explanatory variable puts creates a model that might be more useful to a trebuchet operator, but it also has an important meaning in terms of statistics.  The new variable can help account for some of the distance data, and in so doing can make the model a better fit.  

Many students will be unfamiliar with functions of two variables, simply because they don't encounter them in their mathematics classes.  It's a mistake to think that the standard mathematics curriculum has been designed to teach easy subjects first, and that subjects not encountered in the standard curriculum are somehow more difficult.  But you do have to orient your students to some of the basics of functions of two (or more) variables.

A good place to start is to ask your students to imagine themselves standing on a hillside.  Is the slope the same in all directions?  Many students will respond, "yes."  They have an intuitive notion of the gradient and are thinking that, at each point on the hillside, there is just one slope.  But, as skiers know, the hill is steep in some directions and not at all steep in others.  (Indeed, for every point on every hill, there is a direction where the landscape is flat.)  Looking back at the countour plot, ask the students whether the hill is steeper in the East-West direction (that is, along the $x$-axis), or in the North-South diretion (along the $y$ axis).  It's easy to find the slope in each direction, by taking a small step in that direction and finding the change in altitude.

For the trebuchet distance function, you can take such steps by varying one variable while holding the other constant.  For instance, what's the change in distance (according to the model) when changing the projectile weight by one gram while holding the counter-weight constant.  Pick a value for each variable and tweak projectile weight:
<<tidy=FALSE>>=
ballistic(projectileWt=51,counterWt=1) - 
  ballistic(projectileWt=50,counterWt=1)
@
This indicates that a 1 gm increase in projectile weight is associated with a decrease of 8.5 cm in the distance flown.  

On the other hand, increasing the counter weight is associated with an increase in distance:
<<tidy=FALSE>>=
ballistic(projectileWt=50,counterWt=2) - 
  ballistic(projectileWt=50,counterWt=1)
@
A 1 kg increase in the counter-weight is associated with an increase of 365 cm in the distance.

Show your students the points on the contour plot corresponding to these finite differences.  This helps to understand why each input variable can be associated with a different connection to the output.  Once students understand this, it's easier for them to generalize to functions of more than two variables.  The key thing is to move away from functions of a single variable.

It's well worth observing that the two models, \texttt{mod1} and \texttt{mod2}, give different answers to the question of how the distance changes with a change in the projectile weight.  For \text{mod1} (translated into function \function{f1}) the answer is 
<<>>=
f1(projectileWt=51)-f1(projectileWt=50)
@
But for \texttt{mod2} (translated into function \function{ballistic}) the change in distance is much less, as already seen:
<<tidy=FALSE>>=
ballistic(projectileWt=50,counterWt=2) - 
  ballistic(projectileWt=50,counterWt=1)
@

That the two functions give different answers can be confusing to students.  The difference comes about because the \function{ballistic} function allows you to hold the counter-weight constant while changing the projectile weight.  This is, of course, completely natural --- you can change the projectile weight without changing the counter-weight.  But the experiment happened to be done in such a way that only the lightest projectiles were used with the heaviest counterweights.  
<<>>=
tally( ~ projectileWt + counterWt, data=trebuchet)
@
The result is that the light-weight projectiles flew, on average, much further than the heavier projectiles: partly because the projectiles were lighter and partly because the counter-weight was heavier.

If you're thinking, "Well, the experiment should have been done in a balanced way, with the same range of counter-weights used for each projectile," true enough.  But there experiment wasn't done this way and, as a result, counter-weight and projectile weight have been connected to one another.  To disconnect them, given the data, requires that both counter-weight and projectile weight be used to account for distance flown.

\section{More variables give a better fit}

Remember that your students have been raised mathematically in an environment where there is always a correct answer, sometimes easy to find and sometimes hard.  Many students conflate the difficulty of finding the answer with the quality of the fit --- data that show a clear pattern are easier to deal with than data that don't, and of course the fit is better for the data that show a clear pattern.

Your students will naturally think that fitting a function of two inputs is harder to fit than a function of a single input.  That's fair enough.  For a straight-line function of one variable, it's pretty easy to draw a plausible candidate and to use the techniques of high-school algebra to find the parameters: a slope and intercept.   But it's hard to draw a graph of a function of two variables, let alone use the eye to relate data to the parameters of the fitted function.

Given this focus on the difficulty of the problem, it becomes confusing to see that the function of two explanatory variables must fit the response variable better than a function of one variable.  So, take the time to demonstrate this.  A simple way to see that the two-variable model is a better fit than the one-variable model is to look at the size of the residuals.\marginnote{It's worthwhile to ask your students why the mean of the residuals of a fitted model is not a useful way to characterize their size.  Have them look at the mean residual from different models and figure out what's going on.}  As always, the way to quantify residuals is with their standard deviation, or variance, or sum of squares.
<<>>=
sd(resid(mod1))
sd(resid(mod2))
@


This phenomenon, 
that adding an explanatory variable to a model will reduce the size of the residuals, provides a powerful segue to statistical inference.  For example, you can have your students add a random quantity as a explanatory variable or randomize the explanatory variable by shuffling.  

Danny wants this in a margin note, but it breaks: 
Use \function{rand()} to generate random variables to add into a model, e.g. to add two random variables, use
<<>>=
lm(distance ~ projectileWt + rand(2),data=trebuchet)
@
Shuffling is applied to variables one at a time to randomize them:
<<>>=
lm(distance ~ projectileWt + shuffle(counterWt),data=trebuchet)
@
With these operations, one can implement the null hypothesis rather than just stating it.

The two-variable model also provides a more reliable prediction:
<<>>=
f1( projectileWt=50, interval="prediction")
ballistic( projectileWt=50,counterWt=2, interval="prediction")
@
Notice how much narrower the prediction interval is for the two-variable model compared to the one-variable model.  

\chapter{Functions with Categorical Variables}

In terms of notation, there's not much difference between these two statements:
<<echo=FALSE>>=
print.lm <- function(x) coef(x) # for nicer printing for lm.
@

<<results="hide",tidy=FALSE>>=
vals = mean(distance~object,data=trebuchet)
mod  =   lm(distance~object,data=trebuchet)
@

The first calculates the groupwise means of \VN{distance}, with the groups defined by \VN{object}.
<<>>=
vals = mean(distance~object,data=trebuchet)
vals
@

The second statement fits a model that accounts for the variation in \VN{distance} by the variation in \VN{object}:
<<>>=
mod3 = lm(distance~object,data=trebuchet)
mod3
@
As it happens, the model values from \texttt{mod} coincide directly with the values produced by \function{mean}. To see this, extract the function from the model:
<<>>=
g1 = makeFun(mod3)
g1("foose")
g1("golf")
g1("tennis ball")
@

It may seem odd to your students that a word (or \term{character string}) can be used as the input to a function.  Remind them that a function is a machine that takes one or more inputs and produces an output.  There's no requirement that the inputs be numbers.

The function \function{g1} can be used to calculate a model value for each case in the \texttt{trebuchet} data and, from that, the residuals:
<<>>=
trebuchet = transform(trebuchet,
                      g1resids = distance-g1(object))
@
The size of the residuals can be described, as always, by their standard deviation:
<<>>=
sd(g1resids, data=trebuchet)
@

To see what's special about the model values here --- that is, why the groupwise mean is special --- construct another function that assigns a specific value to each group.  For example, here's a function that says the golf balls go 300 cm, foose balls 500, tennis balls 100 and washers 300.
<<>>=
g2 = makeFun( switch(object,"golf"=300,"foose"=500,
                     "tennis ball"=100,"big washerb"=300, NA) ~ object)
@
This is not the traditional form for a function.  The function \texttt{g2} takes an input \texttt{object} and compares it to the names of the different kinds of balls.  It returns the associated value (or \texttt{NA} if there is no associated value), for instance:
<<>>=
g2("foose")
g2("golf")
g2("tennis ball")
@

Which gives a better description of the distances flown by the various objects, \function{g1} or \function{g2}?  As previously, you can find the residuals from this model and compare them to the residuals from \function{g1}.

The function \function{g2} takes just a single character string.  To allow it to work on a vector of character strings, you can \term{vectorize} it:\authNote{Any way to avoid this?}%
<<>>=
g2 = Vectorize(g2)
@


<<>>=
trebuchet = transform(trebuchet,
                      g2resids = distance-g2(object))
sd(g1resids,data=trebuchet)
sd(g2resids,data=trebuchet)
@

\chapter{From $r$ to $R^2$}

A traditional introductory statistics course will present the correlation coefficient as a way to describe the relationship between two quantitative variables.  As you know, the correlation coefficient is a number between $-1$ and $1$.  A value of $r=0$ means no correlation, a value of $r=1$ means perfect correlation, as does $r=-1$.

As a way of describing relationships among variables, $r$ is not as useful as it might be.  It doesn't handle multiple explanatory variables.  It doesn't handle models with categorical variables.  It doesn't handle nonlinear relationships.

On the other hand, $R^2$ is much more generally useful.  If you are teaching modeling, it makes  sense to introduce $R^2$ as early as possible.  The way to do this is {\bf not} to treat $R^2$ as the square of $r$.  Such a development inherits all the deficiencies of $r$.  Instead, go back to a basic question: How does the model account for the variation in the response variable.

Earlier, the standard deviation was used as a way to quantify the size of residuals.  Let's use it now to quantify the size of variation in the response variable.  For the \VN{distance} variable in the \texttt{trebuchet} data, this is:
<<>>=
sd(distance,data=trebuchet)
@
Now consider the size of the variation in the model values from the various models we have fitted --- \texttt{mod1}, \texttt{mod2}, and \texttt{mod3} from which were extracted the model functions \function{f1}, \function{ballistic}, and \function{g2}:
<<>>=
sd(f1(projectileWt),data=trebuchet)
sd(ballistic(projectileWt,counterWt),data=trebuchet)
sd(g1(object),data=trebuchet)
@
In every case, the size of variation in the fitted model values is {\bf less} than the size of the response variables.  Where's the rest of the variation?  In the residuals.

A fitted model \term{partitions} variation between that accounted for by the model and that which remains unaccounted for.  Measure the variation accounted for using the variation in the fitted model values; measure the rest of the variation using the residuals.  For instance, for \texttt{mod1}:
<<>>=
sd(fitted(mod1))
sd(resid(mod2))
@
It would be nice if the two parts of the variation added up to the whole:
<<>>=
sd(distance,data=trebuchet)
sd(fitted(mod1))+sd(resid(mod2))
@
Alas, that's not exactly true.  The reason, however, is that the standard deviation is not the natural statistic for measuring variation, even if it is the one used in introductory statistics.  Instead, use the square of the standard deviation --- the variance:
<<>>=
var(fitted(mod1))
var(resid(mod1))
var(distance,data=trebuchet)
@
Note that the sum of variances of the fitted model values and the residuals add up exactly to the variance of the response variable.  
<<>>=
var(fitted(mod1))+var(resid(mod1))
@

The situation here is analogous to one your students have encountered before: the Pythagorean theorem: $A^2 + B^2 = C^2$.  It's the square-length of the sides of a right triangle that add in a nice way, not the lengths themselves.  Similarly, the variances add in a nice way, not the standard deviations. 

For more about the analogy between geometry and model fitting, see \cite{bock-velleman}, \cite{kaplan-fresh},\cite{saville}.

The $R^2$ statistic on a model describes what fraction of the variance in the response variable is accounted for by a model.  You can calculate it directly:
<<>>=
var(fitted(mod1))/var(distance,data=trebuchet)
@
For convenience, you can extract the $R^2$ statistic directly from the model, just as you can extract the model function, the fitted values and residuals:
<<>>=
r.squared(mod1)
@

An important question is whether $R^2$ can be used to compare different model designs to decide which is best.  For instance, $R^2$ from the groupwise-mean model \texttt{mod3} is somewhat larger than for \texttt{mod1}:
<<>>=
r.squared(mod3)
@
Does this mean that \texttt{mod3} is better than \texttt{mod1}?  That will turn out to be a productive route to studying statistical inference.  But before heading in that direction, let's expand the set of models that students can build and interpret.

\chapter{Combinations of categorical and quantitative variables}

So far, we've explored these sorts of models:
\begin{itemize}
\item One quantitative explanatory variable.
\item One categorical explanatory variable.
\item Two quantitative explanatory variables.
\end{itemize}

All of these sorts models were constructed with the same syntax and all of them fit into the same framework: explanatory and response variables, fitted model values, residuals, $R^2$.  The syntax and framework extend to more complicated models.  You can add in more explanatory variables using exactly the same syntax.

You can also add in \term{interactions} among explanatory variables.

To illustrate, consider world records in the 100 meter freestyle swimming event as they have changed over the years.  Plot these separately for the two sexes.
<<swim-data,fig.width=5>>=
xyplot(time ~ year | sex, data=SwimRecords)
@

It's evident from the data that, for both sexes the records are improving over time. (How could they not?  That's the nature of a world record.)  The pattern is so clear that one hardly needs a model to interpret it.  But, to display the syntax of models, let's do so anyways.

The record \VN{time} depends on both \VN{sex} and \VN{year}, but it's your choice what explanatory variables to include in a model.  Here are three plausible models:
<<>>=
swim1 = lm(time~sex,data=SwimRecords)
swim2 = lm(time~year,data=SwimRecords)
swim3 = lm(time~year + sex, data=SwimRecords)
@
To plot out the model function, first extract the function from the model:
<<>>=
s1 = makeFun(swim1)
s2 = makeFun(swim2)
s3 = makeFun(swim3)
@

The first model doesn't include \VN{year}.  Still, to graph the model function on the axes, you need to include \VN{year} in the plotting statement. 
<<fig.width=5>>=
<<swim-data>>
plotFun(s1(sex="F")~year,add=TRUE,col="red",lwd=3)
plotFun(s1(sex="M")~year,add=TRUE,col="black")
@
Since the function \function{s1} doesn't depend on \VN{year}, the graphs of the function are flat with respect to \VN{year}.  The function does have \VN{sex} as an input and you can see in the graph how the function values for females (thick line) differ from those for males (thin line).

Now consider \function{s2}, which depends on \function{year} but not \function{sex}.
<<fig.width=5>>=
<<swim-data>>
plotFun(s2(year)~year,add=TRUE,col="red",lwd=3)
plotFun(s2(year)~year,add=TRUE,col="black")
@
The functions are the same for males and females, of course, so they overlie one another on the graph.

Including both \VN{sex} and \VN{year} in the model produces a function that depends on both variables:
<<swim-data>>
plotFun(s3(year=year,sex="F")~year,add=TRUE,col="red",lwd=3)
plotFun(s3(year=year,sex="M")~year,add=TRUE,col="black")
@

You might be surprised to see that the graph of the function for males is parallel to that for females.  That's because there was nothing in the model design that produces a different slope with respect to \VN{year} for females and males. 

Including such a difference in a model is a matter of including an \term{interaction term} between \VN{sex} and \VN{year}:
<<>>=
swim4 = lm(time~year+sex+year:sex,data=SwimRecords)
s4 = makeFun(swim4)
@

<<fig.width=5>>=
<<swim-data>>
plotFun(s4(year=year,sex="F")~year,add=TRUE,col="red",lwd=3)
plotFun(s4(year=year,sex="M")~year,add=TRUE,col="black")
@

Interactions can be confusing at first.  Students tend to want to interpret the word ``interaction'' as meaning that one variable affects the other.  This is not quite right.  An interaction describes how the effect of one variable on the response is modulated by the other variable.  For example, the interaction between \VN{sex} and \VN{year} tells how the relationship between \VN{year} and world-record \VN{time} differs for the two sexes.  You see that interaction in the graph as different slopes for the fitted lines for the two sexes.

Another, way to describe the interaction is that the relationship between \VN{sex} and world-record \VN{time} is changing over the years.  You can see that from the changing vertical distance between the lines for females and males. Both these ways of describing the interaction --- how the relationship between \VN{sex} and \VN{time} is modulated over the years, and how the relationship between \VN{year} and \VN{time} is different for the two sexes --- are equivalent.  Given that the slopes of the two lines is different, the vertical distance between the two lines is going to change.

\section{Example: The Genetic Component of Human Height}

... An example where the relationship is not completely obvious from the graph.  Moving from graphical tools to inferential tools.

the famous data collected by Francis Galton in 19th century London in order to explore the genetic component of human height.  Galton measured the heights of adult children and their parents.  Here's a plot of the child's full-grown height versus the mother's height:
<<>>=
xyplot( height ~ mother | sex, data=Galton)
@

\section{Covariates and Partial Change}

Heading toward Simpson's paradox.

The \texttt{SaratogaHouses.csv} data.

\texttt{SAT} data.

\chapter{Statistical Inference}

Quick resampling example to demonstrate confidence intervals, then use \function{confint}.

ANOVA as a way of tracking $R^2$ as more terms are added.

\chapter{Links to Mathematics}

Notation of functions.  Polynomials in two variables.  Transformation variables.



\chapter{EXERCISES}

These are just sketches for exercises.  Where to deploy them?

\paragraph{One}. Why square instead of taking the absolute value?  There's nothing wrong with mean absolute value as a way of describing the sum of the residuals. Construct the model function where the value is the median of each group.  That function will have a mean absolute value of differences that is as small as any other possible function.

\paragraph{Two}. Do the grand mean.  It's not so easy to think about what the input is.  There is no input.  In this sense, groupwise means are easier to conceive of than grand means.

\paragraph{Three}. Show that \VN{object} gives a perfect prediction of \VN{projectileWt} in the \texttt{trebuchet} data. Does \VN{object} contribute anything beyond \VN{projectileWt} in explaining \VN{distance}?

\end{document}

