<<echo=FALSE,include=FALSE>>=
source('../include/setup.R')
opts_chunk$set( fig.path="figure/Multivariable-fig-" ) 
if (!exists("notAsStandAlone")) set_parent('../include/MainDocument.Rnw')
set.seed(123)
@

\chapter{Multivariable Statistics -- Early?} 
\label{chap:multivariate-early}

Readers of a certain age may remember the 1960s television show {\em
  Lost in Space}.  One of the characters, Robot, was often
assigned guard duty.  Robot would march back and forth, just like a
soldier on guard duty.

But why?  Soldiers are ordered to march back and forth so that they
won't fall asleep; walking forces them to maintain a certain attention
to their duty.  The Robot has no such need; its sensors and circuits can reliably
detect intruders without the marching.  Of course, the television
viewers wouldn't know this about robots.  Using the robot in the
manner of a soldier was a way to introduce new technology to people
with old mind-sets.

Now fast forward from the television of the 1960s to the classroom of
the 21st century.  Students have computers.  They use statistical
software packages to do calculations.  But what calculations are they
doing?  Sample means, sample proportions, differences between means.

This is Robot walking back and forth.  A way to use new technology with
old mind-sets.  But it's the professors, not the students, with the
old mind-sets.  The students are open to new things.  They don't need
to know that, once upon a time, it was an impressive feat to get a machine to add
up a column of numbers.
\authNote{Add pointer to Conrad Wolfram's TED talk?}

Most professors were educated at a time when the tools for inverting a
matrix were paper and pencil, when doing least squares problem involved
a strange thing called a ``pseudo-inverse'' that you might learn in
your fourth or fifth semester of university-level mathematics.  But, now, least squares
problems are no more difficult or time consuming to the human than
square roots or addition.  We just have to learn to use the tools in
the right way.  

Or, rather, we have to show our students what are the
basic operations that are important for statistical reasoning in an
age of modern computation.  Not marching back and forth, like robot
soldiers, computing sums of columns of numbers, but thinking about how to model the
potentially rich interplay among multiple variables.

The standard approach to introductory statistics is based on a few
simple operations: adding, squaring, and square-rooting to finding means 
and standard deviations; 
counting to find proportions and medians/quantiles.  These operations are
(supposed to be) familiar to students from high-school mathematics.

Here we'll examine the consequences of adding in a new basic operation
that is not in the traditional high-school curriculum, but which is
actually quite consistent with the ``Common Core'' curriculum being
introduced by many U.S. states \cite{common-core-2010}.
\authNote{rjp: Fitting simple linear regression models, even with some transformations,
using a TI-calculator seems to be a pretty standard component of junior
and senior high school mathematics these days.}%


The operation is fitting multivariable linear models.  Modern software
makes it no more arduous than finding a mean or standard deviation.
Our emphasis here will be on how to introduce the conceptual
foundations --- using just high-school mathematics and simple extensions
largely specified already in the ``Common Core'' --- that make the concept
of modeling accessible.

There are three important pedagogical advantages to using multivariable linear models
as a foundation for statistics:
\begin{enumerate}
  \item It provides a logically coherent framework that unifies many
    of the disparate techniques found in introductory statistics books.
  \item It allows the very important ideas of confounding and
    covariates to be introduced in an integrated way, showing students
    not only the perils of confounding, but what to do about it (and
    when you can't do anything).
  \item It allows the examples used in classes to be drawn from a
    richer set of situations, and provides scope for students to
    express their creativity and insight.  This can increase student
    motivation.
\authNote{reference Gould paper and specific examples?}
\end{enumerate}
Of course, it's also important that modern statistical work is already
and increasingly shaped by multivariable methods.  For an example of
how over the last few decades statistical methods used in the
literature have shifted away from the curriculum of the traditional,
non-multivariable introductory course, see the review of statistics
practices in the \emph{New England Journal of Medicine}. \cite{switzer-horton-2005}

\section{The Mathematical Foundations}

A standard part of the high-school curriculum is the equation of a
straight line: $y = m x + b$.  Many students will recognize that $m$
is a slope and $b$ is the $y$-intercept.  

It's helpful to move them a bit beyond this:
\authNote{rjp: I've made the list more parallel and separated out the imperative sentences.}%
\begin{itemize}
\item Emphasize the ``function'' concept.  
	
  A function, of course, is a
  relationship between an input and an output.  Generalize this, so
  that students are comfortable with using names other than $X$ as the
  input and
  $Y$ as the output.  For example, 
  $$\mbox{height} = f(\mbox{age}) = 3\ \mbox{age} + 20$$.
  
\item Introduce the idea of functions of non-numeric variables. 
	
The average height may be different for men and women:
$$\mbox{height} = g( \mbox{sex} ) = \left\{ \begin{array}{ll}
      67 & \mbox{for sex $=$ female}\\
      71 & \mbox{for sex $=$ male}\\
      \end{array}\right.$$
   
\item Generalize the idea of a function to include having more than
  one input.
  
Height may well depend on both age and sex:

$$\mbox{height} = h( \mbox{age}, \mbox{sex} ) = -2 * \mbox{age} + 
      \left\{ \begin{array}{ll}
      67 & \mbox{for sex $=$ female}\\
      71 & \mbox{for sex $=$ male}\\
      \end{array}\right.$$

\item  De-program your students from thinking that there is one, and
  only one, correct formula for a relationship.  
  
  Introduce the idea of a function as a
  description of a relationship and that there can be different descriptions of the same
  relationship, all of which can be right in some ways.  Different
  descriptions can include some details and exclude others. Such
  descriptions are called ``models.''  The above models
  $f(\mbox{age})$, and $g(\mbox{sex})$ and $h(\mbox{age}, \mbox{sex})$
  give different outputs for the same inputs.  For example, for a male
  of age 10, according to the models, the height is variously $f(10) =
  50$ or $g(\mbox{male}) = 71$ or $h(10, \mbox{male}) = 51$.
  
\item  Demonstrate the models can be both wrong and useful.
	
  Many useful models don't give exactly the right output for
  every case.  Not every 10-year old male has the same height.  The
  difference between what a model says, and what the value is for an
  actual case, will not in general be zero.  The ``residual'' is the difference between
  the actual value for a given 10-year old male, say Ralph, and what
  a given model says about 10-year old males in general.  The
  residuals give important information about how good a model is.

\authNote{rjp: I think this point is a bit muddled.  If the model is supposed to give average
heights, then no one should expect all people of a given age and sex to have the same height.
But it is probably not the case that all of the \emph{means} fall on our model fit either,
and the model can still be useful.}
\end{itemize}


\section{The Language of Models}
\label{sec:formula}

There is a language for defining models that is different from
writing down an algebraic formula.  

You have already seen some aspects of this notation in graphics
commands, e.g. \verb!height ~ sex!.  You can read this in any of several
ways:
\begin{itemize}
  \item Break down \VN{height} by \VN{sex} (as in a boxplot)
  \item \VN{height} versus \VN{sex}
  \item Model \VN{height} as a function of \VN{sex}.  
\end{itemize}
In this section, you'll see more complicated models, involving
multiple variables.  This makes it worthwhile to assign more precise
labels to the different aspects of the modeling language, which
has just a few components:
\begin{itemize}
  \item The response variable.  This is the output of the model function,
    \VN{height} in the above examples.
  \item The explanatory variables.  These are the inputs to the
    model function: \VN{age} and \VN{sex} in the above examples.
  \item Model terms.  Examples of model terms are explanatory
    variables themselves and a special term called the ``intercept.''
    There are a few others, but we'll deal with them as we come to them.
\end{itemize}
\authNote{Update edition for Kaplan book?}
A more comprehensive introduction is given in Chapters 4 and 5 of \cite{kaplan-2009-book}.)


As an example, consider this simple formula:
\[ 
z = 7 + 3 x + 4.5 y \;.
\]
The corresponding model description consists of the response variable
$z$, and three model terms: the explanatory variables $x$ and
$y$ and the intercept term.  In the modeling language, it would be written:
\begin{quotation}
\centerline{\model{z}{1 + x + y}}
\end{quotation}
Notice that instead of the $=$ sign, we are using the $\sim$ sign.
Also, notice that the specific coefficients $7$, $3$, and $4.5$ are
missing.  The model description is a kind of skeleton for describing
the shape of the model.  It's like saying: ``We want a straight line
model,'' rather than giving the complete specification of the formula.
%\InstructorNote{We're using capital letters for these variables, 
%following a common convention.
%The particular values that these variables take on are generally denoted
%as lower case letters.}
\authNote{I don't think this is a conventional use of anything.  We should avoid 
capitals unless there is a clear random variable involved.  Typically for (basic) regression,
all of the explanatory variables are considered fixed, not random. I've removed the 
instructor note.  We can discuss as needed.}

The process of finding a specific formula to make a model match the
pattern shown by data is called ``fitting the model to the data.''  

To see how different model specifications correspond to different
``shapes'' of models, consider the history of world-record times in the 100-meter 
free-style swimming race.

<<read-swim,echo=FALSE,eval=TRUE>>=
data(SwimRecords)
levels(SwimRecords$sex) <- c('Women','Men')
@

<<showSwimModel, echo=FALSE>>=
showSwimModel <- function(model, plotFormula = time~year, show.legend=FALSE, show.model=TRUE, ...) {
	xyplot(plotFormula, data=SwimRecords, 
				 xlim=c(1899,2010), 
				 xlab='Year',
				 ylab='Time (secs)',
				 groups=sex,
				 auto.key = if (show.legend) list( columns=2 ) else FALSE,
				 par.settings = list( superpose.symbol=list(pch=c(17,15)) ),
				 main= if (show.model) deparse(formula(model)) else "",
				 ...)
	if (show.model) {
		mfit <- makeFun(model)
		plotFun(mfit(year=year, sex='Men') ~ year, add=TRUE)
		plotFun(mfit(year=year, sex='Women') ~ year, add=TRUE)
	}
}
@ 

<<swim-data-raw2,echo=FALSE,eval=TRUE>>=
showSwimModel(show.model=FALSE, show.legend=TRUE)
@ 



You can see the steady improvement in records over the decades from
1900 to the present.  Men's times are somewhat faster than women's.

Now let's build some models.

\subsection{\model{\VN{time}}{\VN{1} + \VN{year}}}

The model \model{\VN{time}}{1 + \VN{year}} gives the familiar
straight-line form: the intercept term (written simply as 1) and a term corresponding to the
linear dependence on \VN{year}.


\authNote{You are surrounded by a mass of redundant plots.  Don't panic.}
\authNote{rjp: I panicked -- and fixed. }
<<swim-data-1b,echo=FALSE>>=
showSwimModel( lm(time ~ year, data=SwimRecords) )
@ 

<<swim-mod-plot3, echo=FALSE>>=
swim.mod.plot3 = function(form, dots=FALSE, show.model=FALSE, lwd=3, show.legend=FALSE,...){
  t = as.character(form)
  t = paste(t[c(2,1,3)],collapse=" ")
  model = lm(form, dat=SwimRecords)
  xyplot(time ~ year, data=SwimRecords, groups=sex, 
	 xlim=c(1899,2010), 
	 xlab='Year',
     ylab='Time (secs)',
	 main=t,
	 auto.key= if (show.legend) list(columns=2) else FALSE,
	 par.settings = list(superpose.symbol=list(pch=c(17,15))),
	 ...)
  if (show.model) {
	  mfit <- makeFun(model)
	  plotFun( mfit(year, sex='Women') ~ year, add=TRUE, col='navy' )
	  plotFun( mfit(year, sex='Men')   ~ year, add=TRUE, col='navy')
  }
}
@

This model captures some of the pattern evident in the data: that
swimming times are improving (getting shorter) over the years.  And it
ignores other obvious features, for example the difference between men's
and women's times, or the curvature reflecting that records are not
improving as fast as they did in the early days.


\subsection{\model{\VN{time}}{\VN{sex}}}

The model \model{\VN{time}}{\VN{sex}} breaks down the swimming times
according to \VN{sex}:

<<swim-data-2,echo=FALSE>>=
showSwimModel( lm(time ~ sex, data=SwimRecords) )
#swim.mod.plot3(time ~ sex)
mfit <- makeFun(lm(time ~ sex, data=SwimRecords))
plotFun( mfit(sex='Women') ~ year, add=TRUE)
plotFun( mfit(sex='Men') ~ year, add=TRUE)
@ 

This model reflects the typical difference between men's and women's
times.  It's oblivious to the trend that records improve over the
years.  Why?  Because the variable \VN{year} was not included in the model.

\subsection{\model{\VN{time}}{\VN{sex} + \VN{year}}}

Neither of the two models shown so far is very satisfactory, but if we allow
multiple variables, we can include both of these important factors (year and sex) 
in one model.
%The record time evidently depends both on \VN{sex} and \VN{year}, so
%it's sensible to include both variables in the model

<<swim-data-3,echo=FALSE>>=
showSwimModel( lm(time ~ sex + year, data=SwimRecords) )
#swim.mod.plot3(time ~ sex + year, dots=FALSE)
mfit <- makeFun(lm(time ~ sex + year, data=SwimRecords))
plotFun( mfit(year, sex='Women') ~ year, add=TRUE)
plotFun( mfit(year, sex='Men') ~ year, add=TRUE)
@ 

\authNote{DTK: Should these graphics be re-written to display as
  continuous forms rather than dots?}
\authNote{rjp: Yes. Done.}

This is a straight-line model with separate lines for the different
sexes. The intercept is different for the different sexes, but the
slope is the same.  
This model is reflects the typical difference between men's and women's
times.  

Students sometimes observe that the function generated by fitting this
model doesn't respect the ``vertical line test'' taught in high-school
algebra.  This is a good time to remind students that this is a
function of \emph{two} variables.  It is indeed a function and for
any specific value of the inputs \VN{sex} and \VN{year} gives a single value.

\subsection{\model{\VN{time}}{\VN{sex} + \VN{year} + \VN{sex}:\VN{year}}}
 
There are two ways that the previous model, \model{\VN{time}}{\VN{sex} + \VN{year}},  misses obvious features in the
data: there is no curvature over the years and the slopes are exactly
the same for men and women.  To construct a model with different
slopes for men and women requires that we add a term that combines
both \VN{sex} and \VN{year}.  Such a term is constructed with the syntax
\VN{sex}:\VN{year} (or, what would amount to the same thing,
\VN{year}:\VN{sex}).  

<<swim-data-4,echo=FALSE>>=
showSwimModel(lm(time ~ sex * year, data=SwimRecords) )
mfit <- makeFun(lm(time ~ sex * year, data=SwimRecords))
plotFun( mfit(year, sex='Women') ~ year, add=TRUE)
plotFun( mfit(year, sex='Men') ~ year, add=TRUE)
@ 

In statistics, such a term is called an \emph{interaction term}.  (In
mathematics, it's often called a \emph{bi-linear term}.)  It's the
term that lets you have different slopes for the different sexes.  

The new phrase ``interaction term'' creates a need for a retronym, a way
to refer to those simple, non-interaction terms that we started with,
like \VN{sex} and \VN{year}.  (Common retronyms in everyday life are acoustic guitar, snail-mail, 
World War I, cloth diaper, and whole milk, compound terms that weren't needed
until electric guitars, e-mail, disposable diapers, and skim milk were
introduced, and World War II showed that the ``War to End All Wars''
was mis-named.)

The standard terminology for terms like \VN{sex} and \VN{year} is unfortunate: ``main effect.'' 
It suggests that interaction terms play a lesser role in modeling.  This is a bad
attitude, since sometimes the interaction is exactly what you're
interested in, but the terminology seems enshrined by statistical tradition.
\authNote{rjp: This discussion is a bit confusing since we are using \variable{year}
and \variable{sex} as both variables and ``terms".
Also, when there is interaction, it is hard to say what one means
by a main effect.}

Very often when you are including an interaction term, you want to
include the main effects as well.  There is a convenient shorthand for
this: \model{\VN{time}}{\VN{sex} * \VN{year}}

\subsection{The Intercept Only: \model{\VN{time}}{\VN{1}}}

It's also possible to have models that have no explanatory variables
whatsoever.  In this case, only the intercept term appears to the right of the
model formula. 

<<swim-data-5,echo=FALSE>>=
showSwimModel(lm(time ~ 1, data=SwimRecords))
mfit <- makeFun(lm(time ~ 1, data=SwimRecords))
plotFun( mfit(year=year)  ~ year, add=TRUE)
@ 

As you might expect, by leaving out both \VN{sex} and \VN{year} from
the model, it doesn't reflect the role of either variable in any way.
But the model \model{\VN{time}}{1} does get one thing very well: the
typical record time.% 
\authNote{rjp:  Is this really true?  It's not clear to me what ``typical record time''
means, and less that this model captures that ``very well''.
}

Think of \model{\VN{time}}{1} as saying ``all the cases are the
same.''%  
\authNote{rjp:  see my previous comment about interpreting models and 
residuals.  Seems like we are making the same error here.}
In some ways, it's analogous to the model
\model{\VN{time}}{\VN{sex}}.  That model says that ``all men are the
same, and all women are the same.''  So the difference between 
\model{\VN{time}}{1} and \model{\VN{time}}{\VN{sex}} is just like the
difference between a ``grand mean'' and a ``group mean.''

\subsection{Transformation Terms}

You can construct more complicated models by adding in more
explanatory variables. (Improved swimming gear?  Better training?
Refinements in technique?).  You can also add in additional terms with
more structure.  There is a rich variety of ways to do this.

Since many students are familiar (or at least remember vaguely) the
idea of quadratics and polynomials, they might be interested to see
that the modeling language can handle this.  Here are three different
models involving a second degree polynomial dependence on \VN{year}:

<<swim-data-6,echo=FALSE>>=
showSwimModel(lm(time ~ poly(year,2), data=SwimRecords) )
mfit <- makeFun(lm(time ~ poly(year, 2), data=SwimRecords))
plotFun( mfit(year) ~ year, add=TRUE)
@ 

<<swim-data-7,echo=FALSE>>=
showSwimModel(lm(time ~ sex + poly(year,2), data=SwimRecords) )
mfit <- makeFun(lm(time ~ sex + poly(year, 2), data=SwimRecords))
plotFun( mfit(year, sex="Women") ~ year, add=TRUE)
plotFun( mfit(year, sex="Men") ~ year, add=TRUE)
@ 

<<swim-data-8,echo=FALSE>>=
showSwimModel(lm(time ~ sex * poly(year,2), data=SwimRecords) )
mfit <- makeFun(lm(time ~ sex * poly(year, 2), data=SwimRecords))
plotFun( mfit(year, sex="Women") ~ year, add=TRUE)
plotFun( mfit(year, sex="Men") ~ year, add=TRUE)
@ 

Although these models reflect more ``detail'' in the data, namely the
curvature, they do it in a way that ultimately does not make sense in
terms of the ``physics'' of world records.
Notice how the upward facing parabolas eventually produce a pattern
where the record times increase over the years.  

There are other sorts of nonlinear terms that might be more
appropriate for modeling this sort of data.  Exponentials, square
roots, etc., even piecewise linear or bent-line forms are all
possible within the modeling framework.  

\section{Helping Students Choose Transformations}

Students typically have very little idea what sorts of transformations 
should be considered candidates for a given situation.
While this sense may develop gradually over time, progress is much quicker if students
are explicitly taught some guidelines for selecting transformations.

\subsection{Theory Driven Transformations}

When theory suggests a non-linear form of relationship between the explanatory and response
variables, it is often possible to linearize the relationship via transformations.

It is a useful exercise to have students linearize relationships such as the following:

\begin{center}
	\renewcommand{\arraystretch}{1.6}
\begin{tabular}{ccc}
	& natural form & linearization
	\\
	\hline
	power law & $ y = \alpha x^{\beta} $ & $\log(y) = \log(\alpha) + \beta \log(x)$
	\\
	product of powers & $ y = \alpha x^{\beta_1} y^{\beta_2} $
		& $\log(y) = \log(\alpha) + \beta_1 \log(x_1) + \beta_2 \log(x_2)$
	\\
	other laws &
	\\
\end{tabular}
\end{center}
Provide students with items in either the center or right column and have them
determine the other.

\subsection{Units}

\subsection{Data-driven Transformations: Tukey's Bulge Rules}

\authNote{rjp: Can grab illustration from FASt and modify exposition.}


\begin{problem}
\authNote{Need to flesh this out}
{\bf DRAFT Outline} for a swim-data modeling problem: construct a post-war
variable and add it in.  Several ways to do this: explore which one gives
models that are most satisfactory to you:
\begin{itemize}
  \item postwar = year $>$ 1945
  \item interaction with postwar and year.
  \item pmax(year - 1945, 0)
\end{itemize}
\end{problem}





\section{Fitting Linear Models to Data in \R} 

Behind the graphical depictions of the models shown in the previous
section is a process for finding specific numerical formulas for the
models.  The software to do this is packaged into the \function{lm()}
function and is very easy for students and professionals alike.  The
human work, and what students need to learn, is not the mechanics of
fitting models, but the interpretation of them.  A good place to start
is with the interpretation of model coefficients.

To illustrate, consider the actual swimming records data employed in
the previous examples.  This data set is available via the internet,
and can be loaded into R with a command like the following:

\authNote{DTK: Eventually, we have to build into MOSAIC an easy way
  for instructors to specify a web location for their data.  The
  \texttt{ISMdata()} function already does this in a limited way.}

<<swim-coeffs1>>=
data(SwimRecords)
@ 
\authNote{rjp:  this data needs to go into the package.}

\authNote{DTK: I couldn't stand the default printing of lm, so I
  changed it. We might want to do this more generally, but it's not
  something we need to worry about here.}

<<redefine-print-lm,echo=FALSE>>=
print.lm <- function (x, digits = max(3, getOption("digits") - 3), ...) 
{
#    cat("\nCall:\n", paste(deparse(x$call), sep = "\n", collapse = "\n"), 
#        "\n\n", sep = "")
    if (length(coef(x))) {
        cat("Coefficients:\n")
        print.default(format(coef(x), digits = digits), print.gap = 2, 
            quote = FALSE)
    }
    else cat("No coefficients\n")
    cat("\n")
    invisible(x)
}
@ 
As before, we'll model the world-record \VN{time} as a function of \VN{sex}
and \VN{year}.

Here's the very simplest model: all cases are modeled as being the same.
<<swim-coeffs2>>=
lm(time ~ 1, data = SwimRecords)
@ 
The fundamental purpose of \texttt{lm()} is to find the coefficients
that flesh out the skeleton provided by the model description.  For
this simple model, the coefficient works out to be the mean of the
record times, which we can calculate a number of different ways.
<<>>=
mean( SwimRecords$time )           # $ interface
mean( ~ time, data=SwimRecords)    # formula interface
mean( time, data=SwimRecords)      # lazy formula interface
@
We can even use a syntax that mirrors the linear model directly:
<<>>=
mean( time ~ 1, data=SwimRecords)  # linear model interface
@ 

Models 
that contain actual explanatory
variables  
are more interesting.
Here's one using the quantitative variable \VN{year}:
<<swim-coeffs3>>=
lm(time ~ year, data = SwimRecords)
@ 
The coefficients now are the slope and intercept of the straight-line
relationship: 
\[
\variable{time}  = -0.2599 * \variable{year} + 567.242\;.
\]

There's a similar story with categorical variables, like \VN{sex}:
<<swim-coeffs4>>=
lm(time ~ sex, data = SwimRecords)
@ 
Based on the result of the simple all-cases-the-same model
\model{\VN{time}}{1}, you might suspect that the coefficients are the
group means.  That's close to being right.  Here are the group means:
<<>>=
mean(time ~ sex, data = SwimRecords)
@ 
\InstructorNote{In the MOSAIC package, the syntax of familiar
  functions like \function{mean()}, \function{sd()}, etc. has been
  extended so that the modeling notation can be used to calculate
  group-by-group values.  Our goal is to make is straightforward to
  transition from conventional basic stats to modeling, by getting
  students used to the $\sim$ and \texttt{data =} notation early.  You
  can even do \texttt{mean(time}$ ~ $ \texttt{1, data = SwimRecords)}. 
  So you can talk about models in a systematic way even if all you want to cover is means,
  proportions, medians, etc.}


The coefficients of the linear model are organized differently from
simple group means.
Notice that there is a \VN{sexM} coefficient, but no similarly named
coefficient for women.  Instead, there is the intercept coefficient.  
This corresponds to the mean \VN{time} of one
of the groups: the reference group.  In this case, the reference group
is women.  

The other coefficient, \VN{sexM} tells the
\emph{difference} between the mean of the reference group and the mean
for the men.  In other words, the coefficients are arranged in
intercept-slope form, but for categorical variables the coefficient
isn't a slope but a finite-difference.  

\InstructorNote{The intercept is so important that the \function{lm()}
  function includes it by default, even if you don't specify it
  explicitly in the model design.  In those rare cases when you don't
  want an intercept term, use the notation \variable{-1} or \variable{0} as part of the
  model design.}

It is possible to fit this model in a different way that does not include
the intercept term (and hence does not present things in terms of 
reference and adjustments):
<<>>=
lm(time ~ 0 + sex, data=SwimRecords)
@
There are good reasons to prefer the model specification that includes the intercept, however,
and we will use that specification in the remainder of our examples.

It's important to keep in mind the intercept-slope/difference format
when interpreting the coefficients from models with multiple
explanatory variables:
<<>>=
lm(time ~ sex + year, data = SwimRecords)
@ 
As usual, there is an intercept coefficient.  It's meaning is a little bit
subtle: it is the intercept for the reference group (in these data, women).  The coefficient
on \VN{year} is a slope.  The \VN{sexM} coefficient is a difference:
it's how the intercept differs for group \VN{M} from the reference
group.  

In traditional mathematical notation, the model formula corresponding
to these coefficients is
$$
\mbox{time} = - 0.2515 * \mbox{year}  + \ \left\{ \begin{array}{rll}
  555.7168  &  & \mbox{for women}\\
555.7168  &  - 9.7980  & \mbox{for men}
\end{array}\right.
$$

\authNote{DTK: For problems, pull out some of the exercises from the
  coefficients chapter, including one on interaction terms. Use Question 1 from the 2011 ISM final
  exam.}

One of the great strengths of R comes from the ability to carry out
new computations on the results from other computations.  To
illustrate, we can save the results from a computation into an object
(in this case \texttt{mod1}).
<<>>=
mod1 <- lm(time ~ year + sex + year:sex, data = SwimRecords)
@ 

From this model object, you can now compute additional information.  Important
functions for demonstrating basic properties of models are
\texttt{resid()},  \texttt{fitted()}, and \texttt{predict()}.  


<<>>=
mean(resid(mod1))
sd(resid(mod1))
var(fitted(mod1))/var(SwimRecords$time) # R-squared
sum(fitted(mod1)^2) # sum of squares of the fitted
sum(resid(mod1)^2) # sum of squares of the residuals
@ 

For plotting the fitted model values
<<>>=
xyplot(fitted(mod1) ~ year, data = SwimRecords)
@ 

The \pkg{mosaic} package includes some additional functions for 
creating a function for computing fitted values of a model and for 
plotting functions.
<<>>=
xyplot(time~year, data=SwimRecords)
mfit1 <- makeFun(mod1)
plotFun( mfit1(year, sex='M') ~ year, add=TRUE)
plotFun( mfit1(year, sex='F') ~ year, add=TRUE)
@


The model suggests that women's times will soon break those of men.
To evaluate the model for new values of inputs, use \function{predict()}
or the function used in our plot above.
<<>>=
predict(mod1, newdata = data.frame(year = 2020, sex = "F"))
predict(mod1, newdata = data.frame(year = 2020, sex = "M"))
@
<<>>=
mfit1(year=2020, sex='F')
mfit1(year=2020, sex='M')
@


When you get to the stage where you want to talk about statistical
inference, you can of course show bootstrapping and permutation
tests.  For example for bootstrapping standard errors:
<<>>=
s <- do(100)* lm(time ~ year + sex + year:sex, data = resample(SwimRecords))
sd(s)
@ 

And, of course, you can do the conventional theoretical calculations
for inference.  
\begin{itemize}
\item Confidence intervals, for instance at a 95\% level
<<>>=
confint(mod1, level = 0.95)
@ 
\item Analysis of variance
<<>>=
anova(mod1)
@ 
\item The regression report and other statistics on the model
<<>>=
summary(mod1)
@ 

\end{itemize}


\section{Example: Genetics before Genes}

To emphasize the flexibility that multivariable models provide to help assess
questions of statistical interest, let's consider a problem of
historical significance: Francis Galton's attempt to quantify the heritability
of height.

Galton famously developed the correlation coefficient in the late
1880s (see \cite{galton-co-relations}.)  One of his motivations was to
put on a quantitative footing the theory of evolution established
by his half-cousin, Charles Darwin.  It's important to realize that
Darwin himself did not know the mechanism by which traits were
inherited.  He imagined a particle of inheritance which he called a
``gemmule.''  The words ``gene'' and ``genetics'' date from the first
decade of the 20th century, the same decade when William Gossett started
publishing under the pseudonym ``Student.''  It wasn't until 1944 when
DNA was shown to be associated with genetic heritability.

Without a mechanism of genotype, Galton needed to rely on what we now
call ``phenotype,'' the observable characteristics themselves.  The 
\pkg{mosaic} package
includes the 
\texttt{Galton} dataset, transcribed to a modern format, of measurements that Galton himself
collected on the heights of adult children and their parents. 
\authNote{Add sidebar about Hanley}



<<galton-data>>=
data(Galton)
head(Galton)
@ 

By today's standards, Galton's techniques were quite limited, but did
suffice to quantify in some ways the heritability of height.  Galton
examined, for the boys, the correlation between the height of the
``mid-parent'' and the boy's height.  If Galton had R available (rather than
having to invent the correlation coefficient $r$!), he might have done the following calculation:
<<>>=
Galton$midparent <- (Galton$father + 1.08*Galton$mother)/2
boys = subset(Galton, sex=="M")
with(boys, cor(midparent,height))
@ 

You may be wondering where the 1.08 comes from in Galton's definition of mid-parent height.  It 
comes from another model!
<<>>=
lm(father~ 0+mother, Galton)
@
\authNote{rjp: added explanation of mid-parent.  Could also redefine mid-parent based on a model
with an intercept -- or perhaps make that an exercise.}

This is purely speculation, but it seems unlikely that Galton,
with interests ranging from exploration of Namibia to fingerprints to meteorology (and, it
must be mentioned, eugenics), would have restricted himself to a
correlation between the mid-parent and boy's heights.  Might height be
inherited differently from the mother and the father?  Is the same
mechanism at work for girls as for boys?  Does one parent's height
have a potentiating effect on the influence of the other parent's height?

Presumably, Galton would have constructed more detailed descriptions
of the relationship between a child's adult height and the genetic and
environmental influences, rather than focusing on only the parent's
height.  Even with the somewhat limited variables in Galton's dataset, one can
use models to explore hypothesized relationships.

By all means, use Galton's data to illustrate simple modeling
techniques, e.g., 
<<tidy=FALSE>>=
model <- lm(height ~ midparent * sex, data = Galton); model
xyplot(height ~ midparent, groups=sex, data=Galton, alpha=.4)
height <- makeFun(model)
plotFun(height(midparent, sex='M') ~ midparent, add=TRUE)
plotFun(height(midparent, sex='F') ~ midparent, add=TRUE)
@ 

<<>>=
lm(height ~ father + sex, data = Galton)
@ 

But consider going further and using inferential methods to see what
evidence is contained in Galton's data more detailed descriptions.
For instance, in the relatively simple model
<<>>=
summary(lm(height ~ sex + father + mother + nkids, data = Galton))
@ 

Ask your students to investigate, through modeling, whether the
influence of the parents is different for the different sexes, or
whether what looks like an even split of influence between the father
and mother could all, in fact, be attributed to the mother.

\begin{problem}
%[stars=1]
Consider the \texttt{CPS85} data that measured per-hour wages and
  other variables for workers in the mid-1980s.  Is there evidence in the data for
  discrimination on the basis of race and/or sex?  What covariates
  might one reasonably adjust for in measuring the difference between
  the races or the sexes?
\end{problem}
  

\shipoutProblems


